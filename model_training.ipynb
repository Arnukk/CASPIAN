{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f89e9e3a-296a-4eb8-92d9-a0c3e16bd343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ak7550\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.10.1; Keras 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import time\n",
    "from models import models as new_models\n",
    "from utils import *\n",
    "import keras\n",
    "from keras_unet_collection import models, utils\n",
    "import matplotlib.pyplot as plt\n",
    "print('TensorFlow {}; Keras {}'.format(tf.__version__, keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f16a1eb-2ea8-470d-a1da-ec6e37692261",
   "metadata": {},
   "source": [
    "## Constants, Parameters and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5846bdd3-227d-4aa7-9e15-8b4285ac3ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the masked points: 12066\n",
      "Size of the Training Dataset: 2240  \n",
      "Size of the Validation Dataset: 240  \n",
      "Training input shape: (1024, 1024, 1)\n",
      "Training input values: [0. 1.]\n",
      "Training output shape: (1024, 1024, 1) \n",
      "Validation input shape: (1024, 1024, 1)\n",
      "Validation output shape: (1024, 1024, 1) \n",
      "Shape of the mask: (2, 1024, 1024)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_size = 1024\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 2\n",
    "split = 1\n",
    "output_1d = False\n",
    "EPOCHS = 200\n",
    "\n",
    "\n",
    "MODEL_NAME = \"Attn-Unet-pretrained\"\n",
    "LR = 0.00015\n",
    "MIN_LR = LR/5\n",
    "WARMUP_EPOCHS = 20\n",
    "\n",
    "MODEL_NAME = \"SWIN-Unet\"\n",
    "LR = 0.00018\n",
    "MIN_LR = LR/6\n",
    "WARMUP_EPOCHS = 20\n",
    "\n",
    "MODEL_NAME = \"CASPIAN\"\n",
    "LR = 0.0008\n",
    "MIN_LR = LR/10\n",
    "WARMUP_EPOCHS = 20\n",
    "\n",
    "\n",
    "###### For Ablation studies ###########################\n",
    "\n",
    "MODEL_NAME = \"CASPIAN_beta_v4\"\n",
    "LR = 0.0008\n",
    "MIN_LR = LR/10\n",
    "WARMUP_EPOCHS = 20\n",
    "version = 4\n",
    "###### For Ablation studies ###########################\n",
    "\n",
    "\n",
    "if not output_1d:\n",
    "    #Loading and processing the mask\n",
    "    the_mask = get_the_mask()\n",
    "\n",
    "\n",
    "#Loading the dataset\n",
    "ds = {\n",
    "    'train': tf.data.Dataset.load(\"./data/train_ds_aug_split_%d\" % split).map(lambda f,x,y,yf: tf.py_function(clear_ds, \n",
    "                                           inp=[f,x,y,yf, output_1d], \n",
    "                                           Tout=[tf.float32, tf.float32])),\n",
    "    'val': tf.data.Dataset.load(\"./data/val_ds_aug_split_%d\" % split).map(lambda f,x,y,yf: tf.py_function(clear_ds, \n",
    "                                           inp=[f,x,y,yf, output_1d], \n",
    "                                           Tout=[tf.float32, tf.float32]))\n",
    "}\n",
    "\n",
    "if \"Attn\" in MODEL_NAME:\n",
    "    ds['train'] = ds['train'].map(lambda x,y: expand_input_channels(x, y))\n",
    "    ds['val'] = ds['val'].map(lambda x,y: expand_input_channels(x, y))\n",
    "\n",
    "print(\"Size of the Training Dataset: %d  \" % tf.data.experimental.cardinality(ds[\"train\"]).numpy())\n",
    "print(\"Size of the Validation Dataset: %d  \" % tf.data.experimental.cardinality(ds[\"val\"]).numpy())\n",
    "input_sample, label_sample = next(iter(ds[\"train\"]))\n",
    "print(\"Training input shape: %s\" % str(input_sample.numpy().shape))\n",
    "print(\"Training input values: %s\" % str(np.unique(input_sample.numpy())))\n",
    "print(\"Training output shape: %s \" % str(label_sample.numpy().shape))\n",
    "input_sample, label_sample = next(iter(ds[\"val\"]))\n",
    "print(\"Validation input shape: %s\" % str(input_sample.numpy().shape))\n",
    "print(\"Validation output shape: %s \" % str(label_sample.numpy().shape))\n",
    "\n",
    "samples = 2\n",
    "fig, axs = plt.subplots(samples,2, facecolor='w', edgecolor='k')\n",
    "#fig.subplots_adjust(hspace = .2, wspace=.13)\n",
    "fig.tight_layout(pad=2, h_pad=2.5, w_pad=2.5)\n",
    "axs = axs.ravel()\n",
    "i = 0\n",
    "for element in list(ds['train'].as_numpy_iterator())[:samples]: \n",
    "    x,y = element\n",
    "    axs[i].imshow(x, origin=\"lower\", interpolation='none', aspect='auto')\n",
    "    axs[i].set_title(\"Sample input\")\n",
    "    i += 1\n",
    "    axs[i].imshow(y, origin=\"lower\", interpolation='none', aspect='auto')\n",
    "    axs[i].set_title(\"Sample output\")\n",
    "    i += 1\n",
    "\n",
    "ds['train'] = ds['train'].batch(batch_size)\n",
    "ds['val'] = ds['val'].batch(batch_size)\n",
    "\n",
    "the_mask = np.array([the_mask]*batch_size)\n",
    "print (\"Shape of the mask: %s\" % str(the_mask.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da54f2b-1030-496e-91c8-eefbb8155119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1024, 1024,  0           []                               \n",
      "                                 1)]                                                              \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1024, 1024,   0           ['input_1[0][0]']                \n",
      "                                1)                                                                \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 1024, 1024,   0           ['lambda[0][0]']                 \n",
      "                                1)                                                                \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 1024, 1024,   0           ['lambda[0][0]']                 \n",
      "                                1)                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1024, 1024,   0           ['lambda_1[0][0]',               \n",
      "                                2)                                'lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 512, 512, 72  360         ['lambda[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 512, 512, 2)  0          ['concatenate[0][0]']            \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512, 512, 74  0           ['conv2d[0][0]',                 \n",
      "                                )                                 'average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 512, 512, 72  5400        ['concatenate_1[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 512, 512, 72  0           ['conv2d[0][0]',                 \n",
      "                                )                                 'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 512, 512, 72  0           ['add[0][0]']                    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " depthwise_conv2d (DepthwiseCon  (None, 256, 256, 72  360        ['activation[0][0]']             \n",
      " v2D)                           )                                                                 \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 256, 256, 2)  0          ['average_pooling2d[0][0]']      \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 256, 256, 74  0           ['depthwise_conv2d[0][0]',       \n",
      "                                )                                 'average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 256, 256, 72  5256        ['activation[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 256, 256, 72  5400        ['concatenate_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 256, 256, 72  0           ['conv2d_3[0][0]',               \n",
      "                                )                                 'conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 256, 256, 72  0           ['add_1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " depthwise_conv2d_1 (DepthwiseC  (None, 128, 128, 72  360        ['activation_1[0][0]']           \n",
      " onv2D)                         )                                                                 \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 128, 128, 2)  0          ['average_pooling2d_1[0][0]']    \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 128, 128, 74  0           ['depthwise_conv2d_1[0][0]',     \n",
      "                                )                                 'average_pooling2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 128, 128, 72  5256        ['activation_1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 128, 128, 72  5400        ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 128, 128, 72  0           ['conv2d_5[0][0]',               \n",
      "                                )                                 'conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 128, 128, 72  0           ['add_2[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " depthwise_conv2d_2 (DepthwiseC  (None, 64, 64, 72)  360         ['activation_2[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (AveragePo  (None, 64, 64, 2)   0           ['average_pooling2d_2[0][0]']    \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 64, 64, 74)   0           ['depthwise_conv2d_2[0][0]',     \n",
      "                                                                  'average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 64, 64, 72)   5256        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 64, 64, 72)   5400        ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 64, 64, 72)   0           ['conv2d_7[0][0]',               \n",
      "                                                                  'conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 64, 64, 72)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 64, 64, 136)  9928        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 64, 64, 136)  5032        ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 64, 64, 72)   9864        ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 64, 64, 72)   0           ['activation_3[0][0]',           \n",
      "                                                                  'conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 64, 64, 72)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 64, 64, 136)  9928        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 64, 64, 136)  5032        ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 64, 64, 72)   9864        ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum (TFOpLambda  (None, 64, 64, 1)   0           ['average_pooling2d_3[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 64, 64, 72)   0           ['activation_4[0][0]',           \n",
      "                                                                  'conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d_4 (AveragePo  (None, 16, 16, 1)   0           ['tf.math.reduce_sum[0][0]']     \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 64, 64, 72)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 256)          0           ['average_pooling2d_4[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 64, 64, 74)   0           ['activation_5[0][0]',           \n",
      "                                                                  'average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 61)           15677       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 128, 128, 72  21384      ['concatenate_5[0][0]']          \n",
      " ose)                           )                                                                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 72)           4464        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 128, 128, 72  5256        ['conv2d_transpose[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 1, 72)     0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 128, 128, 72  0           ['conv2d_14[0][0]',              \n",
      "                                )                                 'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 128, 128, 74  0           ['multiply[0][0]',               \n",
      "                                )                                 'average_pooling2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 256, 256, 72  21384      ['concatenate_6[0][0]']          \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 256, 256, 72  5256        ['conv2d_transpose_1[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 256, 256, 72  0           ['conv2d_15[0][0]',              \n",
      "                                )                                 'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 256, 256, 74  0           ['multiply_1[0][0]',             \n",
      "                                )                                 'average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 512, 512, 72  21384      ['concatenate_7[0][0]']          \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 512, 512, 72  5256        ['conv2d_transpose_2[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)          (None, 512, 512, 72  0           ['conv2d_16[0][0]',              \n",
      "                                )                                 'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 512, 512, 74  0           ['multiply_2[0][0]',             \n",
      "                                )                                 'average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 1024, 1024,   21384      ['concatenate_8[0][0]']          \n",
      " spose)                         72)                                                               \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 1024, 1024,   5256        ['conv2d_transpose_3[0][0]']     \n",
      "                                72)                                                               \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)          (None, 1024, 1024,   0           ['conv2d_17[0][0]',              \n",
      "                                72)                               'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_1 (TFOpLamb  (None, 1024, 1024,   0          ['multiply_3[0][0]']             \n",
      " da)                            1)                                                                \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 1024, 1024,   73          ['multiply_3[0][0]']             \n",
      "                                1)                                                                \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 1024, 1024,   0           ['tf.math.reduce_sum_1[0][0]',   \n",
      "                                1)                                'conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 1024, 1024,   0           ['add_6[0][0]']                  \n",
      "                                1)                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 215,230\n",
      "Trainable params: 215,230\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "   6/1120 [..............................] - ETA: 1:15 - loss: 1.3311 - rmse_score: 3.6884 - mae_score: 2.8124WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0346s vs `on_train_batch_end` time: 0.0719s). Check your callbacks.\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.6396 - rmse_score: 2.2755 - mae_score: 1.4142\n",
      "Epoch 1: val_loss improved from inf to 0.24329, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 99s 76ms/step - loss: 0.6396 - rmse_score: 2.2755 - mae_score: 1.4142 - val_loss: 0.2433 - val_rmse_score: 1.4692 - val_mae_score: 0.5905\n",
      "Epoch 2/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.2423 - rmse_score: 1.4830 - mae_score: 0.5837\n",
      "Epoch 2: val_loss improved from 0.24329 to 0.22748, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.2423 - rmse_score: 1.4830 - mae_score: 0.5837 - val_loss: 0.2275 - val_rmse_score: 1.4398 - val_mae_score: 0.5538\n",
      "Epoch 3/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.2305 - rmse_score: 1.4520 - mae_score: 0.5571\n",
      "Epoch 3: val_loss improved from 0.22748 to 0.21704, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 83s 74ms/step - loss: 0.2305 - rmse_score: 1.4520 - mae_score: 0.5571 - val_loss: 0.2170 - val_rmse_score: 1.4151 - val_mae_score: 0.5294\n",
      "Epoch 4/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.2180 - rmse_score: 1.4166 - mae_score: 0.5259\n",
      "Epoch 4: val_loss improved from 0.21704 to 0.19869, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 83s 74ms/step - loss: 0.2180 - rmse_score: 1.4166 - mae_score: 0.5259 - val_loss: 0.1987 - val_rmse_score: 1.3675 - val_mae_score: 0.4787\n",
      "Epoch 5/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.1937 - rmse_score: 1.3101 - mae_score: 0.4711\n",
      "Epoch 5: val_loss improved from 0.19869 to 0.16012, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 83s 74ms/step - loss: 0.1937 - rmse_score: 1.3101 - mae_score: 0.4711 - val_loss: 0.1601 - val_rmse_score: 1.1981 - val_mae_score: 0.3976\n",
      "Epoch 6/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.1548 - rmse_score: 1.1732 - mae_score: 0.3854\n",
      "Epoch 6: val_loss improved from 0.16012 to 0.14252, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 83s 74ms/step - loss: 0.1548 - rmse_score: 1.1732 - mae_score: 0.3854 - val_loss: 0.1425 - val_rmse_score: 1.1383 - val_mae_score: 0.3689\n",
      "Epoch 7/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.1347 - rmse_score: 1.1009 - mae_score: 0.3415\n",
      "Epoch 7: val_loss improved from 0.14252 to 0.12638, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.1347 - rmse_score: 1.1009 - mae_score: 0.3415 - val_loss: 0.1264 - val_rmse_score: 1.0742 - val_mae_score: 0.3337\n",
      "Epoch 8/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.1179 - rmse_score: 1.0423 - mae_score: 0.3015\n",
      "Epoch 8: val_loss improved from 0.12638 to 0.11636, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 83s 74ms/step - loss: 0.1179 - rmse_score: 1.0423 - mae_score: 0.3015 - val_loss: 0.1164 - val_rmse_score: 1.0309 - val_mae_score: 0.3150\n",
      "Epoch 9/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.1086 - rmse_score: 1.0096 - mae_score: 0.2770\n",
      "Epoch 9: val_loss improved from 0.11636 to 0.10096, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 83s 74ms/step - loss: 0.1086 - rmse_score: 1.0096 - mae_score: 0.2770 - val_loss: 0.1010 - val_rmse_score: 0.9829 - val_mae_score: 0.2642\n",
      "Epoch 10/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.1011 - rmse_score: 0.9777 - mae_score: 0.2585\n",
      "Epoch 10: val_loss improved from 0.10096 to 0.09831, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.1011 - rmse_score: 0.9777 - mae_score: 0.2585 - val_loss: 0.0983 - val_rmse_score: 0.9552 - val_mae_score: 0.2686\n",
      "Epoch 11/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0933 - rmse_score: 0.9372 - mae_score: 0.2414\n",
      "Epoch 11: val_loss improved from 0.09831 to 0.08708, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 83s 74ms/step - loss: 0.0933 - rmse_score: 0.9372 - mae_score: 0.2414 - val_loss: 0.0871 - val_rmse_score: 0.9068 - val_mae_score: 0.2411\n",
      "Epoch 12/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0850 - rmse_score: 0.8999 - mae_score: 0.2192\n",
      "Epoch 12: val_loss improved from 0.08708 to 0.07703, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 83s 74ms/step - loss: 0.0850 - rmse_score: 0.8999 - mae_score: 0.2192 - val_loss: 0.0770 - val_rmse_score: 0.8612 - val_mae_score: 0.2056\n",
      "Epoch 13/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0786 - rmse_score: 0.8664 - mae_score: 0.2045\n",
      "Epoch 13: val_loss did not improve from 0.07703\n",
      "1120/1120 [==============================] - 83s 74ms/step - loss: 0.0786 - rmse_score: 0.8664 - mae_score: 0.2045 - val_loss: 0.0773 - val_rmse_score: 0.8630 - val_mae_score: 0.2036\n",
      "Epoch 14/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0732 - rmse_score: 0.8353 - mae_score: 0.1937\n",
      "Epoch 14: val_loss improved from 0.07703 to 0.06753, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0732 - rmse_score: 0.8353 - mae_score: 0.1937 - val_loss: 0.0675 - val_rmse_score: 0.8053 - val_mae_score: 0.1899\n",
      "Epoch 15/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0696 - rmse_score: 0.8126 - mae_score: 0.1873\n",
      "Epoch 15: val_loss improved from 0.06753 to 0.06338, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0696 - rmse_score: 0.8126 - mae_score: 0.1873 - val_loss: 0.0634 - val_rmse_score: 0.7853 - val_mae_score: 0.1717\n",
      "Epoch 16/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0648 - rmse_score: 0.7897 - mae_score: 0.1709\n",
      "Epoch 16: val_loss improved from 0.06338 to 0.05993, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0648 - rmse_score: 0.7897 - mae_score: 0.1709 - val_loss: 0.0599 - val_rmse_score: 0.7615 - val_mae_score: 0.1622\n",
      "Epoch 17/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0611 - rmse_score: 0.7656 - mae_score: 0.1634\n",
      "Epoch 17: val_loss improved from 0.05993 to 0.05555, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0611 - rmse_score: 0.7656 - mae_score: 0.1634 - val_loss: 0.0555 - val_rmse_score: 0.7339 - val_mae_score: 0.1490\n",
      "Epoch 18/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0566 - rmse_score: 0.7366 - mae_score: 0.1535\n",
      "Epoch 18: val_loss improved from 0.05555 to 0.05325, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0566 - rmse_score: 0.7366 - mae_score: 0.1535 - val_loss: 0.0533 - val_rmse_score: 0.7120 - val_mae_score: 0.1490\n",
      "Epoch 19/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0541 - rmse_score: 0.7187 - mae_score: 0.1480\n",
      "Epoch 19: val_loss improved from 0.05325 to 0.05040, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0541 - rmse_score: 0.7187 - mae_score: 0.1480 - val_loss: 0.0504 - val_rmse_score: 0.6957 - val_mae_score: 0.1415\n",
      "Epoch 20/20\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0514 - rmse_score: 0.6993 - mae_score: 0.1427\n",
      "Epoch 20: val_loss did not improve from 0.05040\n",
      "1120/1120 [==============================] - 83s 74ms/step - loss: 0.0514 - rmse_score: 0.6993 - mae_score: 0.1427 - val_loss: 0.0519 - val_rmse_score: 0.6923 - val_mae_score: 0.1614\n",
      "Epoch 1/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0515 - rmse_score: 0.6983 - mae_score: 0.1440\n",
      "Epoch 1: val_loss improved from 0.05040 to 0.05028, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0515 - rmse_score: 0.6983 - mae_score: 0.1440 - val_loss: 0.0503 - val_rmse_score: 0.6910 - val_mae_score: 0.1408 - lr: 7.5996e-04\n",
      "Epoch 2/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0484 - rmse_score: 0.6809 - mae_score: 0.1335\n",
      "Epoch 2: val_loss did not improve from 0.05028\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0484 - rmse_score: 0.6809 - mae_score: 0.1335 - val_loss: 0.0518 - val_rmse_score: 0.7008 - val_mae_score: 0.1531 - lr: 7.5996e-04\n",
      "Epoch 3/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0471 - rmse_score: 0.6700 - mae_score: 0.1322\n",
      "Epoch 3: val_loss improved from 0.05028 to 0.04572, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0471 - rmse_score: 0.6700 - mae_score: 0.1322 - val_loss: 0.0457 - val_rmse_score: 0.6592 - val_mae_score: 0.1367 - lr: 7.5996e-04\n",
      "Epoch 4/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0449 - rmse_score: 0.6544 - mae_score: 0.1263\n",
      "Epoch 4: val_loss improved from 0.04572 to 0.04565, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0449 - rmse_score: 0.6544 - mae_score: 0.1263 - val_loss: 0.0457 - val_rmse_score: 0.6644 - val_mae_score: 0.1276 - lr: 7.5996e-04\n",
      "Epoch 5/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0436 - rmse_score: 0.6464 - mae_score: 0.1220\n",
      "Epoch 5: val_loss improved from 0.04565 to 0.04322, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0436 - rmse_score: 0.6464 - mae_score: 0.1220 - val_loss: 0.0432 - val_rmse_score: 0.6430 - val_mae_score: 0.1255 - lr: 7.5996e-04\n",
      "Epoch 6/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0417 - rmse_score: 0.6308 - mae_score: 0.1184\n",
      "Epoch 6: val_loss improved from 0.04322 to 0.04221, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 83s 74ms/step - loss: 0.0417 - rmse_score: 0.6308 - mae_score: 0.1184 - val_loss: 0.0422 - val_rmse_score: 0.6363 - val_mae_score: 0.1199 - lr: 7.5996e-04\n",
      "Epoch 7/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0409 - rmse_score: 0.6252 - mae_score: 0.1154\n",
      "Epoch 7: val_loss did not improve from 0.04221\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0409 - rmse_score: 0.6252 - mae_score: 0.1154 - val_loss: 0.0431 - val_rmse_score: 0.6392 - val_mae_score: 0.1274 - lr: 7.5996e-04\n",
      "Epoch 8/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0400 - rmse_score: 0.6176 - mae_score: 0.1135\n",
      "Epoch 8: val_loss improved from 0.04221 to 0.04055, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0400 - rmse_score: 0.6176 - mae_score: 0.1135 - val_loss: 0.0406 - val_rmse_score: 0.6226 - val_mae_score: 0.1137 - lr: 7.5996e-04\n",
      "Epoch 9/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0388 - rmse_score: 0.6064 - mae_score: 0.1114\n",
      "Epoch 9: val_loss did not improve from 0.04055\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0388 - rmse_score: 0.6064 - mae_score: 0.1114 - val_loss: 0.0407 - val_rmse_score: 0.6213 - val_mae_score: 0.1179 - lr: 7.5996e-04\n",
      "Epoch 10/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0373 - rmse_score: 0.5952 - mae_score: 0.1076\n",
      "Epoch 10: val_loss improved from 0.04055 to 0.03900, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0373 - rmse_score: 0.5952 - mae_score: 0.1076 - val_loss: 0.0390 - val_rmse_score: 0.6092 - val_mae_score: 0.1130 - lr: 7.5996e-04\n",
      "Epoch 11/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0361 - rmse_score: 0.5847 - mae_score: 0.1049\n",
      "Epoch 11: val_loss improved from 0.03900 to 0.03805, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0361 - rmse_score: 0.5847 - mae_score: 0.1049 - val_loss: 0.0380 - val_rmse_score: 0.6010 - val_mae_score: 0.1081 - lr: 7.5996e-04\n",
      "Epoch 12/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0355 - rmse_score: 0.5790 - mae_score: 0.1033\n",
      "Epoch 12: val_loss improved from 0.03805 to 0.03726, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 74ms/step - loss: 0.0355 - rmse_score: 0.5790 - mae_score: 0.1033 - val_loss: 0.0373 - val_rmse_score: 0.5939 - val_mae_score: 0.1053 - lr: 7.5996e-04\n",
      "Epoch 13/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0348 - rmse_score: 0.5724 - mae_score: 0.1019\n",
      "Epoch 13: val_loss did not improve from 0.03726\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0348 - rmse_score: 0.5724 - mae_score: 0.1019 - val_loss: 0.0383 - val_rmse_score: 0.6004 - val_mae_score: 0.1112 - lr: 7.5996e-04\n",
      "Epoch 14/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0339 - rmse_score: 0.5657 - mae_score: 0.0994\n",
      "Epoch 14: val_loss did not improve from 0.03726\n",
      "1120/1120 [==============================] - 83s 74ms/step - loss: 0.0339 - rmse_score: 0.5657 - mae_score: 0.0994 - val_loss: 0.0382 - val_rmse_score: 0.6001 - val_mae_score: 0.1132 - lr: 7.5996e-04\n",
      "Epoch 15/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0339 - rmse_score: 0.5652 - mae_score: 0.1000\n",
      "Epoch 15: val_loss did not improve from 0.03726\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0339 - rmse_score: 0.5652 - mae_score: 0.1000 - val_loss: 0.0412 - val_rmse_score: 0.6203 - val_mae_score: 0.1209 - lr: 7.5996e-04\n",
      "Epoch 16/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0333 - rmse_score: 0.5606 - mae_score: 0.0981\n",
      "Epoch 16: val_loss improved from 0.03726 to 0.03710, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0333 - rmse_score: 0.5606 - mae_score: 0.0981 - val_loss: 0.0371 - val_rmse_score: 0.5932 - val_mae_score: 0.1049 - lr: 7.5996e-04\n",
      "Epoch 17/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0324 - rmse_score: 0.5525 - mae_score: 0.0959\n",
      "Epoch 17: val_loss did not improve from 0.03710\n",
      "1120/1120 [==============================] - 81s 73ms/step - loss: 0.0324 - rmse_score: 0.5525 - mae_score: 0.0959 - val_loss: 0.0378 - val_rmse_score: 0.5970 - val_mae_score: 0.1097 - lr: 7.5996e-04\n",
      "Epoch 18/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0326 - rmse_score: 0.5532 - mae_score: 0.0969\n",
      "Epoch 18: val_loss did not improve from 0.03710\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0326 - rmse_score: 0.5532 - mae_score: 0.0969 - val_loss: 0.0375 - val_rmse_score: 0.5960 - val_mae_score: 0.1072 - lr: 7.5996e-04\n",
      "Epoch 19/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0330 - rmse_score: 0.5567 - mae_score: 0.0974\n",
      "Epoch 19: val_loss did not improve from 0.03710\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0330 - rmse_score: 0.5567 - mae_score: 0.0974 - val_loss: 0.0386 - val_rmse_score: 0.6009 - val_mae_score: 0.1116 - lr: 7.5996e-04\n",
      "Epoch 20/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0314 - rmse_score: 0.5437 - mae_score: 0.0934\n",
      "Epoch 20: val_loss improved from 0.03710 to 0.03686, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0314 - rmse_score: 0.5437 - mae_score: 0.0934 - val_loss: 0.0369 - val_rmse_score: 0.5869 - val_mae_score: 0.1083 - lr: 7.5996e-04\n",
      "Epoch 21/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0318 - rmse_score: 0.5463 - mae_score: 0.0950\n",
      "Epoch 21: val_loss did not improve from 0.03686\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0318 - rmse_score: 0.5463 - mae_score: 0.0950 - val_loss: 0.0378 - val_rmse_score: 0.5930 - val_mae_score: 0.1086 - lr: 7.5996e-04\n",
      "Epoch 22/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0306 - rmse_score: 0.5362 - mae_score: 0.0916\n",
      "Epoch 22: val_loss did not improve from 0.03686\n",
      "1120/1120 [==============================] - 81s 73ms/step - loss: 0.0306 - rmse_score: 0.5362 - mae_score: 0.0916 - val_loss: 0.0369 - val_rmse_score: 0.5863 - val_mae_score: 0.1082 - lr: 7.5996e-04\n",
      "Epoch 23/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0304 - rmse_score: 0.5354 - mae_score: 0.0917\n",
      "Epoch 23: val_loss did not improve from 0.03686\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0304 - rmse_score: 0.5354 - mae_score: 0.0917 - val_loss: 0.0382 - val_rmse_score: 0.5993 - val_mae_score: 0.1106 - lr: 7.5996e-04\n",
      "Epoch 24/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0296 - rmse_score: 0.5275 - mae_score: 0.0903\n",
      "Epoch 24: val_loss did not improve from 0.03686\n",
      "1120/1120 [==============================] - 81s 73ms/step - loss: 0.0296 - rmse_score: 0.5275 - mae_score: 0.0903 - val_loss: 0.0378 - val_rmse_score: 0.5942 - val_mae_score: 0.1115 - lr: 7.5996e-04\n",
      "Epoch 25/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0293 - rmse_score: 0.5255 - mae_score: 0.0897\n",
      "Epoch 25: val_loss did not improve from 0.03686\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0293 - rmse_score: 0.5255 - mae_score: 0.0897 - val_loss: 0.0389 - val_rmse_score: 0.6024 - val_mae_score: 0.1156 - lr: 7.5996e-04\n",
      "Epoch 26/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0292 - rmse_score: 0.5235 - mae_score: 0.0895\n",
      "Epoch 26: val_loss did not improve from 0.03686\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0292 - rmse_score: 0.5235 - mae_score: 0.0895 - val_loss: 0.0380 - val_rmse_score: 0.5964 - val_mae_score: 0.1094 - lr: 7.5996e-04\n",
      "Epoch 27/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0286 - rmse_score: 0.5187 - mae_score: 0.0870\n",
      "Epoch 27: val_loss did not improve from 0.03686\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0286 - rmse_score: 0.5187 - mae_score: 0.0870 - val_loss: 0.0376 - val_rmse_score: 0.5921 - val_mae_score: 0.1106 - lr: 7.5996e-04\n",
      "Epoch 28/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0284 - rmse_score: 0.5163 - mae_score: 0.0871\n",
      "Epoch 28: val_loss did not improve from 0.03686\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0284 - rmse_score: 0.5163 - mae_score: 0.0871 - val_loss: 0.0372 - val_rmse_score: 0.5886 - val_mae_score: 0.1096 - lr: 7.5996e-04\n",
      "Epoch 29/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0288 - rmse_score: 0.5200 - mae_score: 0.0883\n",
      "Epoch 29: val_loss did not improve from 0.03686\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0288 - rmse_score: 0.5200 - mae_score: 0.0883 - val_loss: 0.0377 - val_rmse_score: 0.5933 - val_mae_score: 0.1105 - lr: 7.5996e-04\n",
      "Epoch 30/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0277 - rmse_score: 0.5105 - mae_score: 0.0853\n",
      "Epoch 30: val_loss did not improve from 0.03686\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0277 - rmse_score: 0.5105 - mae_score: 0.0853 - val_loss: 0.0379 - val_rmse_score: 0.5960 - val_mae_score: 0.1087 - lr: 7.5996e-04\n",
      "Epoch 31/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0265 - rmse_score: 0.4991 - mae_score: 0.0820\n",
      "Epoch 31: val_loss did not improve from 0.03686\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0265 - rmse_score: 0.4991 - mae_score: 0.0820 - val_loss: 0.0371 - val_rmse_score: 0.5921 - val_mae_score: 0.1054 - lr: 6.4597e-04\n",
      "Epoch 32/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0262 - rmse_score: 0.4957 - mae_score: 0.0813\n",
      "Epoch 32: val_loss improved from 0.03686 to 0.03681, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0262 - rmse_score: 0.4957 - mae_score: 0.0813 - val_loss: 0.0368 - val_rmse_score: 0.5870 - val_mae_score: 0.1064 - lr: 6.4597e-04\n",
      "Epoch 33/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0260 - rmse_score: 0.4932 - mae_score: 0.0813\n",
      "Epoch 33: val_loss improved from 0.03681 to 0.03637, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0260 - rmse_score: 0.4932 - mae_score: 0.0813 - val_loss: 0.0364 - val_rmse_score: 0.5833 - val_mae_score: 0.1044 - lr: 6.4597e-04\n",
      "Epoch 34/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0247 - rmse_score: 0.4804 - mae_score: 0.0785\n",
      "Epoch 34: val_loss improved from 0.03637 to 0.03596, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0247 - rmse_score: 0.4804 - mae_score: 0.0785 - val_loss: 0.0360 - val_rmse_score: 0.5766 - val_mae_score: 0.1065 - lr: 6.4597e-04\n",
      "Epoch 35/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0247 - rmse_score: 0.4806 - mae_score: 0.0785\n",
      "Epoch 35: val_loss improved from 0.03596 to 0.03570, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 82s 73ms/step - loss: 0.0247 - rmse_score: 0.4806 - mae_score: 0.0785 - val_loss: 0.0357 - val_rmse_score: 0.5766 - val_mae_score: 0.1034 - lr: 6.4597e-04\n",
      "Epoch 36/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0242 - rmse_score: 0.4757 - mae_score: 0.0765\n",
      "Epoch 36: val_loss did not improve from 0.03570\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0242 - rmse_score: 0.4757 - mae_score: 0.0765 - val_loss: 0.0359 - val_rmse_score: 0.5786 - val_mae_score: 0.1040 - lr: 6.4597e-04\n",
      "Epoch 37/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0240 - rmse_score: 0.4741 - mae_score: 0.0766\n",
      "Epoch 37: val_loss did not improve from 0.03570\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0240 - rmse_score: 0.4741 - mae_score: 0.0766 - val_loss: 0.0373 - val_rmse_score: 0.5915 - val_mae_score: 0.1082 - lr: 6.4597e-04\n",
      "Epoch 38/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0243 - rmse_score: 0.4766 - mae_score: 0.0775\n",
      "Epoch 38: val_loss improved from 0.03570 to 0.03530, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0243 - rmse_score: 0.4766 - mae_score: 0.0775 - val_loss: 0.0353 - val_rmse_score: 0.5754 - val_mae_score: 0.1022 - lr: 6.4597e-04\n",
      "Epoch 39/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0222 - rmse_score: 0.4543 - mae_score: 0.0725\n",
      "Epoch 39: val_loss improved from 0.03530 to 0.03436, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0222 - rmse_score: 0.4543 - mae_score: 0.0725 - val_loss: 0.0344 - val_rmse_score: 0.5659 - val_mae_score: 0.1016 - lr: 6.4597e-04\n",
      "Epoch 40/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0224 - rmse_score: 0.4565 - mae_score: 0.0729\n",
      "Epoch 40: val_loss did not improve from 0.03436\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0224 - rmse_score: 0.4565 - mae_score: 0.0729 - val_loss: 0.0352 - val_rmse_score: 0.5744 - val_mae_score: 0.1032 - lr: 6.4597e-04\n",
      "Epoch 41/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0220 - rmse_score: 0.4523 - mae_score: 0.0719\n",
      "Epoch 41: val_loss did not improve from 0.03436\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0220 - rmse_score: 0.4523 - mae_score: 0.0719 - val_loss: 0.0346 - val_rmse_score: 0.5697 - val_mae_score: 0.1012 - lr: 6.4597e-04\n",
      "Epoch 42/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0214 - rmse_score: 0.4464 - mae_score: 0.0705\n",
      "Epoch 42: val_loss did not improve from 0.03436\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0214 - rmse_score: 0.4464 - mae_score: 0.0705 - val_loss: 0.0346 - val_rmse_score: 0.5689 - val_mae_score: 0.1017 - lr: 6.4597e-04\n",
      "Epoch 43/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0209 - rmse_score: 0.4408 - mae_score: 0.0691\n",
      "Epoch 43: val_loss did not improve from 0.03436\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0209 - rmse_score: 0.4408 - mae_score: 0.0691 - val_loss: 0.0348 - val_rmse_score: 0.5711 - val_mae_score: 0.1015 - lr: 6.4597e-04\n",
      "Epoch 44/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0217 - rmse_score: 0.4496 - mae_score: 0.0718\n",
      "Epoch 44: val_loss did not improve from 0.03436\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0217 - rmse_score: 0.4496 - mae_score: 0.0718 - val_loss: 0.0375 - val_rmse_score: 0.5912 - val_mae_score: 0.1110 - lr: 6.4597e-04\n",
      "Epoch 45/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0206 - rmse_score: 0.4361 - mae_score: 0.0697\n",
      "Epoch 45: val_loss did not improve from 0.03436\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0206 - rmse_score: 0.4361 - mae_score: 0.0697 - val_loss: 0.0347 - val_rmse_score: 0.5652 - val_mae_score: 0.1092 - lr: 6.4597e-04\n",
      "Epoch 46/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0207 - rmse_score: 0.4373 - mae_score: 0.0708\n",
      "Epoch 46: val_loss improved from 0.03436 to 0.03369, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0207 - rmse_score: 0.4373 - mae_score: 0.0708 - val_loss: 0.0337 - val_rmse_score: 0.5616 - val_mae_score: 0.1010 - lr: 6.4597e-04\n",
      "Epoch 47/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0201 - rmse_score: 0.4315 - mae_score: 0.0686\n",
      "Epoch 47: val_loss did not improve from 0.03369\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0201 - rmse_score: 0.4315 - mae_score: 0.0686 - val_loss: 0.0345 - val_rmse_score: 0.5630 - val_mae_score: 0.1081 - lr: 6.4597e-04\n",
      "Epoch 48/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0193 - rmse_score: 0.4236 - mae_score: 0.0663\n",
      "Epoch 48: val_loss did not improve from 0.03369\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0193 - rmse_score: 0.4236 - mae_score: 0.0663 - val_loss: 0.0341 - val_rmse_score: 0.5653 - val_mae_score: 0.1026 - lr: 6.4597e-04\n",
      "Epoch 49/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0196 - rmse_score: 0.4273 - mae_score: 0.0664\n",
      "Epoch 49: val_loss improved from 0.03369 to 0.03361, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0196 - rmse_score: 0.4273 - mae_score: 0.0664 - val_loss: 0.0336 - val_rmse_score: 0.5609 - val_mae_score: 0.1020 - lr: 6.4597e-04\n",
      "Epoch 50/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0187 - rmse_score: 0.4164 - mae_score: 0.0649\n",
      "Epoch 50: val_loss improved from 0.03361 to 0.03304, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0187 - rmse_score: 0.4164 - mae_score: 0.0649 - val_loss: 0.0330 - val_rmse_score: 0.5561 - val_mae_score: 0.1002 - lr: 6.4597e-04\n",
      "Epoch 51/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0189 - rmse_score: 0.4183 - mae_score: 0.0658\n",
      "Epoch 51: val_loss did not improve from 0.03304\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0189 - rmse_score: 0.4183 - mae_score: 0.0658 - val_loss: 0.0331 - val_rmse_score: 0.5560 - val_mae_score: 0.1008 - lr: 6.4597e-04\n",
      "Epoch 52/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0184 - rmse_score: 0.4121 - mae_score: 0.0645\n",
      "Epoch 52: val_loss improved from 0.03304 to 0.03266, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0184 - rmse_score: 0.4121 - mae_score: 0.0645 - val_loss: 0.0327 - val_rmse_score: 0.5539 - val_mae_score: 0.0985 - lr: 6.4597e-04\n",
      "Epoch 53/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0181 - rmse_score: 0.4075 - mae_score: 0.0645\n",
      "Epoch 53: val_loss did not improve from 0.03266\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0181 - rmse_score: 0.4075 - mae_score: 0.0645 - val_loss: 0.0350 - val_rmse_score: 0.5689 - val_mae_score: 0.1093 - lr: 6.4597e-04\n",
      "Epoch 54/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0176 - rmse_score: 0.4024 - mae_score: 0.0630\n",
      "Epoch 54: val_loss did not improve from 0.03266\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0176 - rmse_score: 0.4024 - mae_score: 0.0630 - val_loss: 0.0333 - val_rmse_score: 0.5541 - val_mae_score: 0.1057 - lr: 6.4597e-04\n",
      "Epoch 55/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0179 - rmse_score: 0.4055 - mae_score: 0.0642\n",
      "Epoch 55: val_loss did not improve from 0.03266\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0179 - rmse_score: 0.4055 - mae_score: 0.0642 - val_loss: 0.0344 - val_rmse_score: 0.5645 - val_mae_score: 0.1097 - lr: 6.4597e-04\n",
      "Epoch 56/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0178 - rmse_score: 0.4028 - mae_score: 0.0651\n",
      "Epoch 56: val_loss improved from 0.03266 to 0.03200, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0178 - rmse_score: 0.4028 - mae_score: 0.0651 - val_loss: 0.0320 - val_rmse_score: 0.5453 - val_mae_score: 0.0978 - lr: 6.4597e-04\n",
      "Epoch 57/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0171 - rmse_score: 0.3971 - mae_score: 0.0613\n",
      "Epoch 57: val_loss improved from 0.03200 to 0.03197, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0171 - rmse_score: 0.3971 - mae_score: 0.0613 - val_loss: 0.0320 - val_rmse_score: 0.5463 - val_mae_score: 0.0973 - lr: 6.4597e-04\n",
      "Epoch 58/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0174 - rmse_score: 0.4013 - mae_score: 0.0615\n",
      "Epoch 58: val_loss did not improve from 0.03197\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0174 - rmse_score: 0.4013 - mae_score: 0.0615 - val_loss: 0.0329 - val_rmse_score: 0.5597 - val_mae_score: 0.0979 - lr: 6.4597e-04\n",
      "Epoch 59/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0163 - rmse_score: 0.3889 - mae_score: 0.0582\n",
      "Epoch 59: val_loss improved from 0.03197 to 0.03188, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0163 - rmse_score: 0.3889 - mae_score: 0.0582 - val_loss: 0.0319 - val_rmse_score: 0.5469 - val_mae_score: 0.0962 - lr: 6.4597e-04\n",
      "Epoch 60/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0160 - rmse_score: 0.3852 - mae_score: 0.0577\n",
      "Epoch 60: val_loss did not improve from 0.03188\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0160 - rmse_score: 0.3852 - mae_score: 0.0577 - val_loss: 0.0336 - val_rmse_score: 0.5614 - val_mae_score: 0.1012 - lr: 6.4597e-04\n",
      "Epoch 61/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0165 - rmse_score: 0.3900 - mae_score: 0.0593\n",
      "Epoch 61: val_loss improved from 0.03188 to 0.03182, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0165 - rmse_score: 0.3900 - mae_score: 0.0593 - val_loss: 0.0318 - val_rmse_score: 0.5471 - val_mae_score: 0.0958 - lr: 6.4597e-04\n",
      "Epoch 62/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0163 - rmse_score: 0.3874 - mae_score: 0.0586\n",
      "Epoch 62: val_loss improved from 0.03182 to 0.03163, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0163 - rmse_score: 0.3874 - mae_score: 0.0586 - val_loss: 0.0316 - val_rmse_score: 0.5481 - val_mae_score: 0.0945 - lr: 6.4597e-04\n",
      "Epoch 63/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0162 - rmse_score: 0.3861 - mae_score: 0.0591\n",
      "Epoch 63: val_loss did not improve from 0.03163\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0162 - rmse_score: 0.3861 - mae_score: 0.0591 - val_loss: 0.0321 - val_rmse_score: 0.5492 - val_mae_score: 0.0978 - lr: 6.4597e-04\n",
      "Epoch 64/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0155 - rmse_score: 0.3769 - mae_score: 0.0580\n",
      "Epoch 64: val_loss did not improve from 0.03163\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0155 - rmse_score: 0.3769 - mae_score: 0.0580 - val_loss: 0.0321 - val_rmse_score: 0.5465 - val_mae_score: 0.0993 - lr: 6.4597e-04\n",
      "Epoch 65/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0152 - rmse_score: 0.3738 - mae_score: 0.0569\n",
      "Epoch 65: val_loss improved from 0.03163 to 0.03147, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0152 - rmse_score: 0.3738 - mae_score: 0.0569 - val_loss: 0.0315 - val_rmse_score: 0.5429 - val_mae_score: 0.0964 - lr: 6.4597e-04\n",
      "Epoch 66/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0164 - rmse_score: 0.3873 - mae_score: 0.0601\n",
      "Epoch 66: val_loss did not improve from 0.03147\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0164 - rmse_score: 0.3873 - mae_score: 0.0601 - val_loss: 0.0317 - val_rmse_score: 0.5425 - val_mae_score: 0.0970 - lr: 6.4597e-04\n",
      "Epoch 67/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0154 - rmse_score: 0.3766 - mae_score: 0.0571\n",
      "Epoch 67: val_loss improved from 0.03147 to 0.03109, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0154 - rmse_score: 0.3766 - mae_score: 0.0571 - val_loss: 0.0311 - val_rmse_score: 0.5393 - val_mae_score: 0.0944 - lr: 6.4597e-04\n",
      "Epoch 68/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0155 - rmse_score: 0.3770 - mae_score: 0.0569\n",
      "Epoch 68: val_loss improved from 0.03109 to 0.03084, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0155 - rmse_score: 0.3770 - mae_score: 0.0569 - val_loss: 0.0308 - val_rmse_score: 0.5374 - val_mae_score: 0.0936 - lr: 6.4597e-04\n",
      "Epoch 69/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0156 - rmse_score: 0.3790 - mae_score: 0.0573\n",
      "Epoch 69: val_loss did not improve from 0.03084\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0156 - rmse_score: 0.3790 - mae_score: 0.0573 - val_loss: 0.0319 - val_rmse_score: 0.5490 - val_mae_score: 0.0965 - lr: 6.4597e-04\n",
      "Epoch 70/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0155 - rmse_score: 0.3760 - mae_score: 0.0573\n",
      "Epoch 70: val_loss did not improve from 0.03084\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0155 - rmse_score: 0.3760 - mae_score: 0.0573 - val_loss: 0.0316 - val_rmse_score: 0.5446 - val_mae_score: 0.0965 - lr: 6.4597e-04\n",
      "Epoch 71/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0145 - rmse_score: 0.3643 - mae_score: 0.0539\n",
      "Epoch 71: val_loss improved from 0.03084 to 0.03057, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0145 - rmse_score: 0.3643 - mae_score: 0.0539 - val_loss: 0.0306 - val_rmse_score: 0.5365 - val_mae_score: 0.0928 - lr: 6.4597e-04\n",
      "Epoch 72/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0141 - rmse_score: 0.3586 - mae_score: 0.0537\n",
      "Epoch 72: val_loss did not improve from 0.03057\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0141 - rmse_score: 0.3586 - mae_score: 0.0537 - val_loss: 0.0308 - val_rmse_score: 0.5378 - val_mae_score: 0.0943 - lr: 6.4597e-04\n",
      "Epoch 73/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0143 - rmse_score: 0.3610 - mae_score: 0.0542\n",
      "Epoch 73: val_loss did not improve from 0.03057\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0143 - rmse_score: 0.3610 - mae_score: 0.0542 - val_loss: 0.0319 - val_rmse_score: 0.5497 - val_mae_score: 0.0961 - lr: 6.4597e-04\n",
      "Epoch 74/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0138 - rmse_score: 0.3546 - mae_score: 0.0531\n",
      "Epoch 74: val_loss did not improve from 0.03057\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0138 - rmse_score: 0.3546 - mae_score: 0.0531 - val_loss: 0.0307 - val_rmse_score: 0.5374 - val_mae_score: 0.0935 - lr: 6.4597e-04\n",
      "Epoch 75/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0135 - rmse_score: 0.3508 - mae_score: 0.0526\n",
      "Epoch 75: val_loss did not improve from 0.03057\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0135 - rmse_score: 0.3508 - mae_score: 0.0526 - val_loss: 0.0306 - val_rmse_score: 0.5365 - val_mae_score: 0.0934 - lr: 6.4597e-04\n",
      "Epoch 76/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0148 - rmse_score: 0.3662 - mae_score: 0.0558\n",
      "Epoch 76: val_loss improved from 0.03057 to 0.02968, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0148 - rmse_score: 0.3662 - mae_score: 0.0558 - val_loss: 0.0297 - val_rmse_score: 0.5279 - val_mae_score: 0.0906 - lr: 6.4597e-04\n",
      "Epoch 77/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0133 - rmse_score: 0.3487 - mae_score: 0.0516\n",
      "Epoch 77: val_loss did not improve from 0.02968\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0133 - rmse_score: 0.3487 - mae_score: 0.0516 - val_loss: 0.0298 - val_rmse_score: 0.5297 - val_mae_score: 0.0918 - lr: 6.4597e-04\n",
      "Epoch 78/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0133 - rmse_score: 0.3478 - mae_score: 0.0514\n",
      "Epoch 78: val_loss did not improve from 0.02968\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0133 - rmse_score: 0.3478 - mae_score: 0.0514 - val_loss: 0.0304 - val_rmse_score: 0.5374 - val_mae_score: 0.0920 - lr: 6.4597e-04\n",
      "Epoch 79/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0136 - rmse_score: 0.3518 - mae_score: 0.0523\n",
      "Epoch 79: val_loss did not improve from 0.02968\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0136 - rmse_score: 0.3518 - mae_score: 0.0523 - val_loss: 0.0316 - val_rmse_score: 0.5470 - val_mae_score: 0.0961 - lr: 6.4597e-04\n",
      "Epoch 80/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0136 - rmse_score: 0.3524 - mae_score: 0.0526\n",
      "Epoch 80: val_loss improved from 0.02968 to 0.02957, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0136 - rmse_score: 0.3524 - mae_score: 0.0526 - val_loss: 0.0296 - val_rmse_score: 0.5303 - val_mae_score: 0.0895 - lr: 6.4597e-04\n",
      "Epoch 81/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0134 - rmse_score: 0.3484 - mae_score: 0.0520\n",
      "Epoch 81: val_loss did not improve from 0.02957\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0134 - rmse_score: 0.3484 - mae_score: 0.0520 - val_loss: 0.0322 - val_rmse_score: 0.5501 - val_mae_score: 0.0978 - lr: 6.4597e-04\n",
      "Epoch 82/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0137 - rmse_score: 0.3524 - mae_score: 0.0527\n",
      "Epoch 82: val_loss did not improve from 0.02957\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0137 - rmse_score: 0.3524 - mae_score: 0.0527 - val_loss: 0.0304 - val_rmse_score: 0.5350 - val_mae_score: 0.0919 - lr: 6.4597e-04\n",
      "Epoch 83/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0138 - rmse_score: 0.3530 - mae_score: 0.0530\n",
      "Epoch 83: val_loss did not improve from 0.02957\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0138 - rmse_score: 0.3530 - mae_score: 0.0530 - val_loss: 0.0314 - val_rmse_score: 0.5442 - val_mae_score: 0.0949 - lr: 6.4597e-04\n",
      "Epoch 84/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0132 - rmse_score: 0.3475 - mae_score: 0.0514\n",
      "Epoch 84: val_loss did not improve from 0.02957\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0132 - rmse_score: 0.3475 - mae_score: 0.0514 - val_loss: 0.0300 - val_rmse_score: 0.5328 - val_mae_score: 0.0900 - lr: 6.4597e-04\n",
      "Epoch 85/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0133 - rmse_score: 0.3467 - mae_score: 0.0518\n",
      "Epoch 85: val_loss did not improve from 0.02957\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0133 - rmse_score: 0.3467 - mae_score: 0.0518 - val_loss: 0.0298 - val_rmse_score: 0.5316 - val_mae_score: 0.0902 - lr: 6.4597e-04\n",
      "Epoch 86/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0132 - rmse_score: 0.3453 - mae_score: 0.0516\n",
      "Epoch 86: val_loss did not improve from 0.02957\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0132 - rmse_score: 0.3453 - mae_score: 0.0516 - val_loss: 0.0310 - val_rmse_score: 0.5381 - val_mae_score: 0.0964 - lr: 6.4597e-04\n",
      "Epoch 87/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0137 - rmse_score: 0.3526 - mae_score: 0.0531\n",
      "Epoch 87: val_loss did not improve from 0.02957\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0137 - rmse_score: 0.3526 - mae_score: 0.0531 - val_loss: 0.0301 - val_rmse_score: 0.5340 - val_mae_score: 0.0907 - lr: 6.4597e-04\n",
      "Epoch 88/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0128 - rmse_score: 0.3411 - mae_score: 0.0510\n",
      "Epoch 88: val_loss did not improve from 0.02957\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0128 - rmse_score: 0.3411 - mae_score: 0.0510 - val_loss: 0.0297 - val_rmse_score: 0.5265 - val_mae_score: 0.0920 - lr: 6.4597e-04\n",
      "Epoch 89/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0131 - rmse_score: 0.3425 - mae_score: 0.0527\n",
      "Epoch 89: val_loss did not improve from 0.02957\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0131 - rmse_score: 0.3425 - mae_score: 0.0527 - val_loss: 0.0310 - val_rmse_score: 0.5387 - val_mae_score: 0.0968 - lr: 6.4597e-04\n",
      "Epoch 90/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0132 - rmse_score: 0.3460 - mae_score: 0.0526\n",
      "Epoch 90: val_loss did not improve from 0.02957\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0132 - rmse_score: 0.3460 - mae_score: 0.0526 - val_loss: 0.0300 - val_rmse_score: 0.5308 - val_mae_score: 0.0917 - lr: 6.4597e-04\n",
      "Epoch 91/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0123 - rmse_score: 0.3348 - mae_score: 0.0484\n",
      "Epoch 91: val_loss improved from 0.02957 to 0.02933, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0123 - rmse_score: 0.3348 - mae_score: 0.0484 - val_loss: 0.0293 - val_rmse_score: 0.5269 - val_mae_score: 0.0885 - lr: 5.4907e-04\n",
      "Epoch 92/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0121 - rmse_score: 0.3310 - mae_score: 0.0477\n",
      "Epoch 92: val_loss did not improve from 0.02933\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0121 - rmse_score: 0.3310 - mae_score: 0.0477 - val_loss: 0.0302 - val_rmse_score: 0.5352 - val_mae_score: 0.0897 - lr: 5.4907e-04\n",
      "Epoch 93/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0121 - rmse_score: 0.3311 - mae_score: 0.0479\n",
      "Epoch 93: val_loss did not improve from 0.02933\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0121 - rmse_score: 0.3311 - mae_score: 0.0479 - val_loss: 0.0299 - val_rmse_score: 0.5327 - val_mae_score: 0.0895 - lr: 5.4907e-04\n",
      "Epoch 94/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0120 - rmse_score: 0.3299 - mae_score: 0.0476\n",
      "Epoch 94: val_loss improved from 0.02933 to 0.02913, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0120 - rmse_score: 0.3299 - mae_score: 0.0476 - val_loss: 0.0291 - val_rmse_score: 0.5259 - val_mae_score: 0.0872 - lr: 5.4907e-04\n",
      "Epoch 95/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0121 - rmse_score: 0.3317 - mae_score: 0.0482\n",
      "Epoch 95: val_loss did not improve from 0.02913\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0121 - rmse_score: 0.3317 - mae_score: 0.0482 - val_loss: 0.0296 - val_rmse_score: 0.5297 - val_mae_score: 0.0890 - lr: 5.4907e-04\n",
      "Epoch 96/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0116 - rmse_score: 0.3242 - mae_score: 0.0464\n",
      "Epoch 96: val_loss did not improve from 0.02913\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0116 - rmse_score: 0.3242 - mae_score: 0.0464 - val_loss: 0.0295 - val_rmse_score: 0.5286 - val_mae_score: 0.0893 - lr: 5.4907e-04\n",
      "Epoch 97/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0117 - rmse_score: 0.3267 - mae_score: 0.0468\n",
      "Epoch 97: val_loss did not improve from 0.02913\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0117 - rmse_score: 0.3267 - mae_score: 0.0468 - val_loss: 0.0294 - val_rmse_score: 0.5264 - val_mae_score: 0.0881 - lr: 5.4907e-04\n",
      "Epoch 98/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0115 - rmse_score: 0.3233 - mae_score: 0.0464\n",
      "Epoch 98: val_loss did not improve from 0.02913\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0115 - rmse_score: 0.3233 - mae_score: 0.0464 - val_loss: 0.0302 - val_rmse_score: 0.5334 - val_mae_score: 0.0909 - lr: 5.4907e-04\n",
      "Epoch 99/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0114 - rmse_score: 0.3213 - mae_score: 0.0458\n",
      "Epoch 99: val_loss did not improve from 0.02913\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0114 - rmse_score: 0.3213 - mae_score: 0.0458 - val_loss: 0.0304 - val_rmse_score: 0.5377 - val_mae_score: 0.0905 - lr: 5.4907e-04\n",
      "Epoch 100/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0117 - rmse_score: 0.3255 - mae_score: 0.0468\n",
      "Epoch 100: val_loss did not improve from 0.02913\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0117 - rmse_score: 0.3255 - mae_score: 0.0468 - val_loss: 0.0299 - val_rmse_score: 0.5336 - val_mae_score: 0.0887 - lr: 5.4907e-04\n",
      "Epoch 101/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0122 - rmse_score: 0.3323 - mae_score: 0.0482\n",
      "Epoch 101: val_loss did not improve from 0.02913\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0122 - rmse_score: 0.3323 - mae_score: 0.0482 - val_loss: 0.0302 - val_rmse_score: 0.5373 - val_mae_score: 0.0895 - lr: 5.4907e-04\n",
      "Epoch 102/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0112 - rmse_score: 0.3187 - mae_score: 0.0453\n",
      "Epoch 102: val_loss did not improve from 0.02913\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0112 - rmse_score: 0.3187 - mae_score: 0.0453 - val_loss: 0.0296 - val_rmse_score: 0.5298 - val_mae_score: 0.0889 - lr: 5.4907e-04\n",
      "Epoch 103/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0111 - rmse_score: 0.3172 - mae_score: 0.0451\n",
      "Epoch 103: val_loss did not improve from 0.02913\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0111 - rmse_score: 0.3172 - mae_score: 0.0451 - val_loss: 0.0298 - val_rmse_score: 0.5312 - val_mae_score: 0.0894 - lr: 5.4907e-04\n",
      "Epoch 104/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0113 - rmse_score: 0.3199 - mae_score: 0.0456\n",
      "Epoch 104: val_loss did not improve from 0.02913\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0113 - rmse_score: 0.3199 - mae_score: 0.0456 - val_loss: 0.0295 - val_rmse_score: 0.5282 - val_mae_score: 0.0883 - lr: 5.4907e-04\n",
      "Epoch 105/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0108 - rmse_score: 0.3131 - mae_score: 0.0440\n",
      "Epoch 105: val_loss did not improve from 0.02913\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0108 - rmse_score: 0.3131 - mae_score: 0.0440 - val_loss: 0.0292 - val_rmse_score: 0.5256 - val_mae_score: 0.0870 - lr: 4.6671e-04\n",
      "Epoch 106/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0107 - rmse_score: 0.3109 - mae_score: 0.0435\n",
      "Epoch 106: val_loss did not improve from 0.02913\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0107 - rmse_score: 0.3109 - mae_score: 0.0435 - val_loss: 0.0305 - val_rmse_score: 0.5378 - val_mae_score: 0.0912 - lr: 4.6671e-04\n",
      "Epoch 107/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0106 - rmse_score: 0.3092 - mae_score: 0.0432\n",
      "Epoch 107: val_loss did not improve from 0.02913\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0106 - rmse_score: 0.3092 - mae_score: 0.0432 - val_loss: 0.0293 - val_rmse_score: 0.5264 - val_mae_score: 0.0873 - lr: 4.6671e-04\n",
      "Epoch 108/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0105 - rmse_score: 0.3080 - mae_score: 0.0429\n",
      "Epoch 108: val_loss improved from 0.02913 to 0.02895, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0105 - rmse_score: 0.3080 - mae_score: 0.0429 - val_loss: 0.0289 - val_rmse_score: 0.5233 - val_mae_score: 0.0865 - lr: 4.6671e-04\n",
      "Epoch 109/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0108 - rmse_score: 0.3124 - mae_score: 0.0438\n",
      "Epoch 109: val_loss did not improve from 0.02895\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0108 - rmse_score: 0.3124 - mae_score: 0.0438 - val_loss: 0.0295 - val_rmse_score: 0.5288 - val_mae_score: 0.0891 - lr: 4.6671e-04\n",
      "Epoch 110/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0106 - rmse_score: 0.3095 - mae_score: 0.0436\n",
      "Epoch 110: val_loss did not improve from 0.02895\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0106 - rmse_score: 0.3095 - mae_score: 0.0436 - val_loss: 0.0293 - val_rmse_score: 0.5255 - val_mae_score: 0.0890 - lr: 4.6671e-04\n",
      "Epoch 111/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0104 - rmse_score: 0.3072 - mae_score: 0.0430\n",
      "Epoch 111: val_loss did not improve from 0.02895\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0104 - rmse_score: 0.3072 - mae_score: 0.0430 - val_loss: 0.0295 - val_rmse_score: 0.5296 - val_mae_score: 0.0877 - lr: 4.6671e-04\n",
      "Epoch 112/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0110 - rmse_score: 0.3151 - mae_score: 0.0445\n",
      "Epoch 112: val_loss did not improve from 0.02895\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0110 - rmse_score: 0.3151 - mae_score: 0.0445 - val_loss: 0.0292 - val_rmse_score: 0.5265 - val_mae_score: 0.0867 - lr: 4.6671e-04\n",
      "Epoch 113/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0105 - rmse_score: 0.3088 - mae_score: 0.0430\n",
      "Epoch 113: val_loss did not improve from 0.02895\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0105 - rmse_score: 0.3088 - mae_score: 0.0430 - val_loss: 0.0294 - val_rmse_score: 0.5281 - val_mae_score: 0.0876 - lr: 4.6671e-04\n",
      "Epoch 114/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0101 - rmse_score: 0.3034 - mae_score: 0.0419\n",
      "Epoch 114: val_loss improved from 0.02895 to 0.02858, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0101 - rmse_score: 0.3034 - mae_score: 0.0419 - val_loss: 0.0286 - val_rmse_score: 0.5218 - val_mae_score: 0.0851 - lr: 4.6671e-04\n",
      "Epoch 115/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0107 - rmse_score: 0.3108 - mae_score: 0.0436\n",
      "Epoch 115: val_loss did not improve from 0.02858\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0107 - rmse_score: 0.3108 - mae_score: 0.0436 - val_loss: 0.0298 - val_rmse_score: 0.5315 - val_mae_score: 0.0885 - lr: 4.6671e-04\n",
      "Epoch 116/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0102 - rmse_score: 0.3037 - mae_score: 0.0423\n",
      "Epoch 116: val_loss improved from 0.02858 to 0.02835, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0102 - rmse_score: 0.3037 - mae_score: 0.0423 - val_loss: 0.0284 - val_rmse_score: 0.5202 - val_mae_score: 0.0853 - lr: 4.6671e-04\n",
      "Epoch 117/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0098 - rmse_score: 0.2966 - mae_score: 0.0411\n",
      "Epoch 117: val_loss did not improve from 0.02835\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0098 - rmse_score: 0.2966 - mae_score: 0.0411 - val_loss: 0.0288 - val_rmse_score: 0.5234 - val_mae_score: 0.0859 - lr: 4.6671e-04\n",
      "Epoch 118/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0097 - rmse_score: 0.2961 - mae_score: 0.0410\n",
      "Epoch 118: val_loss did not improve from 0.02835\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0097 - rmse_score: 0.2961 - mae_score: 0.0410 - val_loss: 0.0288 - val_rmse_score: 0.5234 - val_mae_score: 0.0859 - lr: 4.6671e-04\n",
      "Epoch 119/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0104 - rmse_score: 0.3061 - mae_score: 0.0433\n",
      "Epoch 119: val_loss did not improve from 0.02835\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0104 - rmse_score: 0.3061 - mae_score: 0.0433 - val_loss: 0.0292 - val_rmse_score: 0.5265 - val_mae_score: 0.0880 - lr: 4.6671e-04\n",
      "Epoch 120/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0097 - rmse_score: 0.2957 - mae_score: 0.0412\n",
      "Epoch 120: val_loss did not improve from 0.02835\n",
      "1120/1120 [==============================] - 81s 73ms/step - loss: 0.0097 - rmse_score: 0.2957 - mae_score: 0.0412 - val_loss: 0.0289 - val_rmse_score: 0.5247 - val_mae_score: 0.0861 - lr: 4.6671e-04\n",
      "Epoch 121/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0097 - rmse_score: 0.2955 - mae_score: 0.0409\n",
      "Epoch 121: val_loss improved from 0.02835 to 0.02800, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0097 - rmse_score: 0.2955 - mae_score: 0.0409 - val_loss: 0.0280 - val_rmse_score: 0.5159 - val_mae_score: 0.0840 - lr: 4.6671e-04\n",
      "Epoch 122/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0100 - rmse_score: 0.2994 - mae_score: 0.0419\n",
      "Epoch 122: val_loss did not improve from 0.02800\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0100 - rmse_score: 0.2994 - mae_score: 0.0419 - val_loss: 0.0285 - val_rmse_score: 0.5201 - val_mae_score: 0.0848 - lr: 4.6671e-04\n",
      "Epoch 123/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0098 - rmse_score: 0.2972 - mae_score: 0.0415\n",
      "Epoch 123: val_loss did not improve from 0.02800\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0098 - rmse_score: 0.2972 - mae_score: 0.0415 - val_loss: 0.0293 - val_rmse_score: 0.5280 - val_mae_score: 0.0872 - lr: 4.6671e-04\n",
      "Epoch 124/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0098 - rmse_score: 0.2961 - mae_score: 0.0412\n",
      "Epoch 124: val_loss did not improve from 0.02800\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0098 - rmse_score: 0.2961 - mae_score: 0.0412 - val_loss: 0.0286 - val_rmse_score: 0.5216 - val_mae_score: 0.0851 - lr: 4.6671e-04\n",
      "Epoch 125/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0100 - rmse_score: 0.3002 - mae_score: 0.0418\n",
      "Epoch 125: val_loss did not improve from 0.02800\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0100 - rmse_score: 0.3002 - mae_score: 0.0418 - val_loss: 0.0283 - val_rmse_score: 0.5177 - val_mae_score: 0.0846 - lr: 4.6671e-04\n",
      "Epoch 126/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0098 - rmse_score: 0.2961 - mae_score: 0.0412\n",
      "Epoch 126: val_loss did not improve from 0.02800\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0098 - rmse_score: 0.2961 - mae_score: 0.0412 - val_loss: 0.0283 - val_rmse_score: 0.5185 - val_mae_score: 0.0841 - lr: 4.6671e-04\n",
      "Epoch 127/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0097 - rmse_score: 0.2946 - mae_score: 0.0409\n",
      "Epoch 127: val_loss did not improve from 0.02800\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0097 - rmse_score: 0.2946 - mae_score: 0.0409 - val_loss: 0.0298 - val_rmse_score: 0.5326 - val_mae_score: 0.0879 - lr: 4.6671e-04\n",
      "Epoch 128/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0098 - rmse_score: 0.2962 - mae_score: 0.0416\n",
      "Epoch 128: val_loss did not improve from 0.02800\n",
      "1120/1120 [==============================] - 81s 73ms/step - loss: 0.0098 - rmse_score: 0.2962 - mae_score: 0.0416 - val_loss: 0.0287 - val_rmse_score: 0.5240 - val_mae_score: 0.0855 - lr: 4.6671e-04\n",
      "Epoch 129/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0098 - rmse_score: 0.2959 - mae_score: 0.0414\n",
      "Epoch 129: val_loss did not improve from 0.02800\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0098 - rmse_score: 0.2959 - mae_score: 0.0414 - val_loss: 0.0287 - val_rmse_score: 0.5224 - val_mae_score: 0.0854 - lr: 4.6671e-04\n",
      "Epoch 130/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0093 - rmse_score: 0.2889 - mae_score: 0.0400\n",
      "Epoch 130: val_loss did not improve from 0.02800\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0093 - rmse_score: 0.2889 - mae_score: 0.0400 - val_loss: 0.0291 - val_rmse_score: 0.5252 - val_mae_score: 0.0873 - lr: 4.6671e-04\n",
      "Epoch 131/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0094 - rmse_score: 0.2901 - mae_score: 0.0404\n",
      "Epoch 131: val_loss did not improve from 0.02800\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0094 - rmse_score: 0.2901 - mae_score: 0.0404 - val_loss: 0.0288 - val_rmse_score: 0.5224 - val_mae_score: 0.0873 - lr: 4.6671e-04\n",
      "Epoch 132/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0091 - rmse_score: 0.2858 - mae_score: 0.0393\n",
      "Epoch 132: val_loss did not improve from 0.02800\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0091 - rmse_score: 0.2858 - mae_score: 0.0393 - val_loss: 0.0291 - val_rmse_score: 0.5249 - val_mae_score: 0.0866 - lr: 3.9671e-04\n",
      "Epoch 133/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0093 - rmse_score: 0.2887 - mae_score: 0.0396\n",
      "Epoch 133: val_loss did not improve from 0.02800\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0093 - rmse_score: 0.2887 - mae_score: 0.0396 - val_loss: 0.0304 - val_rmse_score: 0.5392 - val_mae_score: 0.0887 - lr: 3.9671e-04\n",
      "Epoch 134/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0091 - rmse_score: 0.2856 - mae_score: 0.0388\n",
      "Epoch 134: val_loss did not improve from 0.02800\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0091 - rmse_score: 0.2856 - mae_score: 0.0388 - val_loss: 0.0284 - val_rmse_score: 0.5209 - val_mae_score: 0.0834 - lr: 3.9671e-04\n",
      "Epoch 135/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0089 - rmse_score: 0.2820 - mae_score: 0.0382\n",
      "Epoch 135: val_loss did not improve from 0.02800\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0089 - rmse_score: 0.2820 - mae_score: 0.0382 - val_loss: 0.0283 - val_rmse_score: 0.5196 - val_mae_score: 0.0834 - lr: 3.9671e-04\n",
      "Epoch 136/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0090 - rmse_score: 0.2838 - mae_score: 0.0386\n",
      "Epoch 136: val_loss improved from 0.02800 to 0.02794, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0090 - rmse_score: 0.2838 - mae_score: 0.0386 - val_loss: 0.0279 - val_rmse_score: 0.5169 - val_mae_score: 0.0825 - lr: 3.9671e-04\n",
      "Epoch 137/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0087 - rmse_score: 0.2800 - mae_score: 0.0377\n",
      "Epoch 137: val_loss improved from 0.02794 to 0.02789, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0087 - rmse_score: 0.2800 - mae_score: 0.0377 - val_loss: 0.0279 - val_rmse_score: 0.5161 - val_mae_score: 0.0823 - lr: 3.9671e-04\n",
      "Epoch 138/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0090 - rmse_score: 0.2840 - mae_score: 0.0385\n",
      "Epoch 138: val_loss did not improve from 0.02789\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0090 - rmse_score: 0.2840 - mae_score: 0.0385 - val_loss: 0.0286 - val_rmse_score: 0.5218 - val_mae_score: 0.0840 - lr: 3.9671e-04\n",
      "Epoch 139/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0087 - rmse_score: 0.2795 - mae_score: 0.0377\n",
      "Epoch 139: val_loss did not improve from 0.02789\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0087 - rmse_score: 0.2795 - mae_score: 0.0377 - val_loss: 0.0285 - val_rmse_score: 0.5210 - val_mae_score: 0.0837 - lr: 3.9671e-04\n",
      "Epoch 140/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0090 - rmse_score: 0.2839 - mae_score: 0.0385\n",
      "Epoch 140: val_loss did not improve from 0.02789\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0090 - rmse_score: 0.2839 - mae_score: 0.0385 - val_loss: 0.0290 - val_rmse_score: 0.5252 - val_mae_score: 0.0844 - lr: 3.9671e-04\n",
      "Epoch 141/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0086 - rmse_score: 0.2777 - mae_score: 0.0376\n",
      "Epoch 141: val_loss did not improve from 0.02789\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0086 - rmse_score: 0.2777 - mae_score: 0.0376 - val_loss: 0.0281 - val_rmse_score: 0.5176 - val_mae_score: 0.0829 - lr: 3.9671e-04\n",
      "Epoch 142/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0084 - rmse_score: 0.2754 - mae_score: 0.0369\n",
      "Epoch 142: val_loss did not improve from 0.02789\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0084 - rmse_score: 0.2754 - mae_score: 0.0369 - val_loss: 0.0279 - val_rmse_score: 0.5165 - val_mae_score: 0.0818 - lr: 3.9671e-04\n",
      "Epoch 143/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0087 - rmse_score: 0.2793 - mae_score: 0.0376\n",
      "Epoch 143: val_loss did not improve from 0.02789\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0087 - rmse_score: 0.2793 - mae_score: 0.0376 - val_loss: 0.0285 - val_rmse_score: 0.5215 - val_mae_score: 0.0834 - lr: 3.9671e-04\n",
      "Epoch 144/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0086 - rmse_score: 0.2775 - mae_score: 0.0376\n",
      "Epoch 144: val_loss did not improve from 0.02789\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0086 - rmse_score: 0.2775 - mae_score: 0.0376 - val_loss: 0.0282 - val_rmse_score: 0.5202 - val_mae_score: 0.0830 - lr: 3.9671e-04\n",
      "Epoch 145/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0086 - rmse_score: 0.2770 - mae_score: 0.0375\n",
      "Epoch 145: val_loss did not improve from 0.02789\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0086 - rmse_score: 0.2770 - mae_score: 0.0375 - val_loss: 0.0282 - val_rmse_score: 0.5192 - val_mae_score: 0.0830 - lr: 3.9671e-04\n",
      "Epoch 146/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0087 - rmse_score: 0.2785 - mae_score: 0.0378\n",
      "Epoch 146: val_loss did not improve from 0.02789\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0087 - rmse_score: 0.2785 - mae_score: 0.0378 - val_loss: 0.0283 - val_rmse_score: 0.5207 - val_mae_score: 0.0832 - lr: 3.9671e-04\n",
      "Epoch 147/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0084 - rmse_score: 0.2732 - mae_score: 0.0370\n",
      "Epoch 147: val_loss did not improve from 0.02789\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0084 - rmse_score: 0.2732 - mae_score: 0.0370 - val_loss: 0.0282 - val_rmse_score: 0.5178 - val_mae_score: 0.0832 - lr: 3.9671e-04\n",
      "Epoch 148/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0080 - rmse_score: 0.2670 - mae_score: 0.0358\n",
      "Epoch 148: val_loss improved from 0.02789 to 0.02760, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0080 - rmse_score: 0.2670 - mae_score: 0.0358 - val_loss: 0.0276 - val_rmse_score: 0.5137 - val_mae_score: 0.0808 - lr: 3.3720e-04\n",
      "Epoch 149/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0081 - rmse_score: 0.2688 - mae_score: 0.0360\n",
      "Epoch 149: val_loss did not improve from 0.02760\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0081 - rmse_score: 0.2688 - mae_score: 0.0360 - val_loss: 0.0282 - val_rmse_score: 0.5190 - val_mae_score: 0.0824 - lr: 3.3720e-04\n",
      "Epoch 150/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0078 - rmse_score: 0.2642 - mae_score: 0.0352\n",
      "Epoch 150: val_loss did not improve from 0.02760\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0078 - rmse_score: 0.2642 - mae_score: 0.0352 - val_loss: 0.0277 - val_rmse_score: 0.5156 - val_mae_score: 0.0807 - lr: 3.3720e-04\n",
      "Epoch 151/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0080 - rmse_score: 0.2671 - mae_score: 0.0360\n",
      "Epoch 151: val_loss did not improve from 0.02760\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0080 - rmse_score: 0.2671 - mae_score: 0.0360 - val_loss: 0.0280 - val_rmse_score: 0.5178 - val_mae_score: 0.0825 - lr: 3.3720e-04\n",
      "Epoch 152/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0080 - rmse_score: 0.2679 - mae_score: 0.0359\n",
      "Epoch 152: val_loss did not improve from 0.02760\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0080 - rmse_score: 0.2679 - mae_score: 0.0359 - val_loss: 0.0284 - val_rmse_score: 0.5223 - val_mae_score: 0.0830 - lr: 3.3720e-04\n",
      "Epoch 153/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0079 - rmse_score: 0.2652 - mae_score: 0.0353\n",
      "Epoch 153: val_loss did not improve from 0.02760\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0079 - rmse_score: 0.2652 - mae_score: 0.0353 - val_loss: 0.0277 - val_rmse_score: 0.5154 - val_mae_score: 0.0811 - lr: 3.3720e-04\n",
      "Epoch 154/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0079 - rmse_score: 0.2656 - mae_score: 0.0352\n",
      "Epoch 154: val_loss did not improve from 0.02760\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0079 - rmse_score: 0.2656 - mae_score: 0.0352 - val_loss: 0.0280 - val_rmse_score: 0.5180 - val_mae_score: 0.0821 - lr: 3.3720e-04\n",
      "Epoch 155/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0077 - rmse_score: 0.2624 - mae_score: 0.0345\n",
      "Epoch 155: val_loss did not improve from 0.02760\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0077 - rmse_score: 0.2624 - mae_score: 0.0345 - val_loss: 0.0279 - val_rmse_score: 0.5175 - val_mae_score: 0.0814 - lr: 3.3720e-04\n",
      "Epoch 156/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0079 - rmse_score: 0.2668 - mae_score: 0.0353\n",
      "Epoch 156: val_loss improved from 0.02760 to 0.02727, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0079 - rmse_score: 0.2668 - mae_score: 0.0353 - val_loss: 0.0273 - val_rmse_score: 0.5117 - val_mae_score: 0.0793 - lr: 3.3720e-04\n",
      "Epoch 157/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0077 - rmse_score: 0.2636 - mae_score: 0.0346\n",
      "Epoch 157: val_loss did not improve from 0.02727\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0077 - rmse_score: 0.2636 - mae_score: 0.0346 - val_loss: 0.0275 - val_rmse_score: 0.5135 - val_mae_score: 0.0800 - lr: 3.3720e-04\n",
      "Epoch 158/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0078 - rmse_score: 0.2634 - mae_score: 0.0347\n",
      "Epoch 158: val_loss did not improve from 0.02727\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0078 - rmse_score: 0.2634 - mae_score: 0.0347 - val_loss: 0.0275 - val_rmse_score: 0.5121 - val_mae_score: 0.0800 - lr: 3.3720e-04\n",
      "Epoch 159/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0079 - rmse_score: 0.2664 - mae_score: 0.0349\n",
      "Epoch 159: val_loss improved from 0.02727 to 0.02723, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0079 - rmse_score: 0.2664 - mae_score: 0.0349 - val_loss: 0.0272 - val_rmse_score: 0.5101 - val_mae_score: 0.0792 - lr: 3.3720e-04\n",
      "Epoch 160/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0078 - rmse_score: 0.2647 - mae_score: 0.0348\n",
      "Epoch 160: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0078 - rmse_score: 0.2647 - mae_score: 0.0348 - val_loss: 0.0280 - val_rmse_score: 0.5186 - val_mae_score: 0.0811 - lr: 3.3720e-04\n",
      "Epoch 161/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0080 - rmse_score: 0.2668 - mae_score: 0.0354\n",
      "Epoch 161: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0080 - rmse_score: 0.2668 - mae_score: 0.0354 - val_loss: 0.0278 - val_rmse_score: 0.5161 - val_mae_score: 0.0807 - lr: 3.3720e-04\n",
      "Epoch 162/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0077 - rmse_score: 0.2633 - mae_score: 0.0347\n",
      "Epoch 162: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0077 - rmse_score: 0.2633 - mae_score: 0.0347 - val_loss: 0.0281 - val_rmse_score: 0.5187 - val_mae_score: 0.0811 - lr: 3.3720e-04\n",
      "Epoch 163/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0076 - rmse_score: 0.2600 - mae_score: 0.0342\n",
      "Epoch 163: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0076 - rmse_score: 0.2600 - mae_score: 0.0342 - val_loss: 0.0275 - val_rmse_score: 0.5142 - val_mae_score: 0.0801 - lr: 3.3720e-04\n",
      "Epoch 164/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0076 - rmse_score: 0.2599 - mae_score: 0.0341\n",
      "Epoch 164: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0076 - rmse_score: 0.2599 - mae_score: 0.0341 - val_loss: 0.0276 - val_rmse_score: 0.5132 - val_mae_score: 0.0803 - lr: 3.3720e-04\n",
      "Epoch 165/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0077 - rmse_score: 0.2617 - mae_score: 0.0346\n",
      "Epoch 165: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0077 - rmse_score: 0.2617 - mae_score: 0.0346 - val_loss: 0.0277 - val_rmse_score: 0.5138 - val_mae_score: 0.0811 - lr: 3.3720e-04\n",
      "Epoch 166/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0077 - rmse_score: 0.2619 - mae_score: 0.0346\n",
      "Epoch 166: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0077 - rmse_score: 0.2619 - mae_score: 0.0346 - val_loss: 0.0279 - val_rmse_score: 0.5170 - val_mae_score: 0.0815 - lr: 3.3720e-04\n",
      "Epoch 167/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0075 - rmse_score: 0.2588 - mae_score: 0.0339\n",
      "Epoch 167: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0075 - rmse_score: 0.2588 - mae_score: 0.0339 - val_loss: 0.0279 - val_rmse_score: 0.5163 - val_mae_score: 0.0810 - lr: 2.8662e-04\n",
      "Epoch 168/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0073 - rmse_score: 0.2557 - mae_score: 0.0331\n",
      "Epoch 168: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0073 - rmse_score: 0.2557 - mae_score: 0.0331 - val_loss: 0.0276 - val_rmse_score: 0.5132 - val_mae_score: 0.0799 - lr: 2.8662e-04\n",
      "Epoch 169/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0074 - rmse_score: 0.2566 - mae_score: 0.0335\n",
      "Epoch 169: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0074 - rmse_score: 0.2566 - mae_score: 0.0335 - val_loss: 0.0279 - val_rmse_score: 0.5165 - val_mae_score: 0.0804 - lr: 2.8662e-04\n",
      "Epoch 170/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0073 - rmse_score: 0.2557 - mae_score: 0.0333\n",
      "Epoch 170: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0073 - rmse_score: 0.2557 - mae_score: 0.0333 - val_loss: 0.0278 - val_rmse_score: 0.5161 - val_mae_score: 0.0804 - lr: 2.8662e-04\n",
      "Epoch 171/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0072 - rmse_score: 0.2538 - mae_score: 0.0329\n",
      "Epoch 171: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0072 - rmse_score: 0.2538 - mae_score: 0.0329 - val_loss: 0.0275 - val_rmse_score: 0.5140 - val_mae_score: 0.0795 - lr: 2.8662e-04\n",
      "Epoch 172/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0072 - rmse_score: 0.2530 - mae_score: 0.0325\n",
      "Epoch 172: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0072 - rmse_score: 0.2530 - mae_score: 0.0325 - val_loss: 0.0279 - val_rmse_score: 0.5170 - val_mae_score: 0.0801 - lr: 2.8662e-04\n",
      "Epoch 173/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0074 - rmse_score: 0.2572 - mae_score: 0.0334\n",
      "Epoch 173: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0074 - rmse_score: 0.2572 - mae_score: 0.0334 - val_loss: 0.0299 - val_rmse_score: 0.5372 - val_mae_score: 0.0856 - lr: 2.8662e-04\n",
      "Epoch 174/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0072 - rmse_score: 0.2533 - mae_score: 0.0327\n",
      "Epoch 174: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0072 - rmse_score: 0.2533 - mae_score: 0.0327 - val_loss: 0.0274 - val_rmse_score: 0.5127 - val_mae_score: 0.0790 - lr: 2.8662e-04\n",
      "Epoch 175/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0072 - rmse_score: 0.2532 - mae_score: 0.0328\n",
      "Epoch 175: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0072 - rmse_score: 0.2532 - mae_score: 0.0328 - val_loss: 0.0275 - val_rmse_score: 0.5133 - val_mae_score: 0.0796 - lr: 2.8662e-04\n",
      "Epoch 176/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0071 - rmse_score: 0.2523 - mae_score: 0.0325\n",
      "Epoch 176: val_loss did not improve from 0.02723\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0071 - rmse_score: 0.2523 - mae_score: 0.0325 - val_loss: 0.0281 - val_rmse_score: 0.5196 - val_mae_score: 0.0812 - lr: 2.8662e-04\n",
      "Epoch 177/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0072 - rmse_score: 0.2539 - mae_score: 0.0328\n",
      "Epoch 177: val_loss improved from 0.02723 to 0.02720, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0072 - rmse_score: 0.2539 - mae_score: 0.0328 - val_loss: 0.0272 - val_rmse_score: 0.5107 - val_mae_score: 0.0786 - lr: 2.4363e-04\n",
      "Epoch 178/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0072 - rmse_score: 0.2527 - mae_score: 0.0324\n",
      "Epoch 178: val_loss did not improve from 0.02720\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0072 - rmse_score: 0.2527 - mae_score: 0.0324 - val_loss: 0.0276 - val_rmse_score: 0.5149 - val_mae_score: 0.0798 - lr: 2.4363e-04\n",
      "Epoch 179/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0070 - rmse_score: 0.2494 - mae_score: 0.0318\n",
      "Epoch 179: val_loss improved from 0.02720 to 0.02717, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0070 - rmse_score: 0.2494 - mae_score: 0.0318 - val_loss: 0.0272 - val_rmse_score: 0.5111 - val_mae_score: 0.0781 - lr: 2.4363e-04\n",
      "Epoch 180/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0069 - rmse_score: 0.2476 - mae_score: 0.0313\n",
      "Epoch 180: val_loss did not improve from 0.02717\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0069 - rmse_score: 0.2476 - mae_score: 0.0313 - val_loss: 0.0277 - val_rmse_score: 0.5151 - val_mae_score: 0.0794 - lr: 2.4363e-04\n",
      "Epoch 181/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0068 - rmse_score: 0.2468 - mae_score: 0.0312\n",
      "Epoch 181: val_loss did not improve from 0.02717\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0068 - rmse_score: 0.2468 - mae_score: 0.0312 - val_loss: 0.0272 - val_rmse_score: 0.5110 - val_mae_score: 0.0783 - lr: 2.4363e-04\n",
      "Epoch 182/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0069 - rmse_score: 0.2487 - mae_score: 0.0317\n",
      "Epoch 182: val_loss improved from 0.02717 to 0.02712, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 81s 72ms/step - loss: 0.0069 - rmse_score: 0.2487 - mae_score: 0.0317 - val_loss: 0.0271 - val_rmse_score: 0.5100 - val_mae_score: 0.0780 - lr: 2.4363e-04\n",
      "Epoch 183/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0068 - rmse_score: 0.2473 - mae_score: 0.0314\n",
      "Epoch 183: val_loss did not improve from 0.02712\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0068 - rmse_score: 0.2473 - mae_score: 0.0314 - val_loss: 0.0275 - val_rmse_score: 0.5139 - val_mae_score: 0.0791 - lr: 2.4363e-04\n",
      "Epoch 184/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0068 - rmse_score: 0.2463 - mae_score: 0.0311\n",
      "Epoch 184: val_loss did not improve from 0.02712\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0068 - rmse_score: 0.2463 - mae_score: 0.0311 - val_loss: 0.0273 - val_rmse_score: 0.5114 - val_mae_score: 0.0784 - lr: 2.4363e-04\n",
      "Epoch 185/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0069 - rmse_score: 0.2474 - mae_score: 0.0314\n",
      "Epoch 185: val_loss did not improve from 0.02712\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0069 - rmse_score: 0.2474 - mae_score: 0.0314 - val_loss: 0.0272 - val_rmse_score: 0.5112 - val_mae_score: 0.0782 - lr: 2.4363e-04\n",
      "Epoch 186/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0068 - rmse_score: 0.2466 - mae_score: 0.0312\n",
      "Epoch 186: val_loss improved from 0.02712 to 0.02691, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0068 - rmse_score: 0.2466 - mae_score: 0.0312 - val_loss: 0.0269 - val_rmse_score: 0.5091 - val_mae_score: 0.0776 - lr: 2.4363e-04\n",
      "Epoch 187/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0068 - rmse_score: 0.2468 - mae_score: 0.0312\n",
      "Epoch 187: val_loss did not improve from 0.02691\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0068 - rmse_score: 0.2468 - mae_score: 0.0312 - val_loss: 0.0277 - val_rmse_score: 0.5152 - val_mae_score: 0.0798 - lr: 2.4363e-04\n",
      "Epoch 188/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0068 - rmse_score: 0.2473 - mae_score: 0.0313\n",
      "Epoch 188: val_loss did not improve from 0.02691\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0068 - rmse_score: 0.2473 - mae_score: 0.0313 - val_loss: 0.0273 - val_rmse_score: 0.5123 - val_mae_score: 0.0783 - lr: 2.4363e-04\n",
      "Epoch 189/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0068 - rmse_score: 0.2470 - mae_score: 0.0311\n",
      "Epoch 189: val_loss did not improve from 0.02691\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0068 - rmse_score: 0.2470 - mae_score: 0.0311 - val_loss: 0.0272 - val_rmse_score: 0.5118 - val_mae_score: 0.0782 - lr: 2.4363e-04\n",
      "Epoch 190/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0067 - rmse_score: 0.2453 - mae_score: 0.0308\n",
      "Epoch 190: val_loss did not improve from 0.02691\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0067 - rmse_score: 0.2453 - mae_score: 0.0308 - val_loss: 0.0273 - val_rmse_score: 0.5128 - val_mae_score: 0.0785 - lr: 2.4363e-04\n",
      "Epoch 191/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0068 - rmse_score: 0.2460 - mae_score: 0.0309\n",
      "Epoch 191: val_loss did not improve from 0.02691\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0068 - rmse_score: 0.2460 - mae_score: 0.0309 - val_loss: 0.0273 - val_rmse_score: 0.5126 - val_mae_score: 0.0782 - lr: 2.4363e-04\n",
      "Epoch 192/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0067 - rmse_score: 0.2451 - mae_score: 0.0308\n",
      "Epoch 192: val_loss did not improve from 0.02691\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0067 - rmse_score: 0.2451 - mae_score: 0.0308 - val_loss: 0.0276 - val_rmse_score: 0.5140 - val_mae_score: 0.0790 - lr: 2.4363e-04\n",
      "Epoch 193/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0068 - rmse_score: 0.2458 - mae_score: 0.0310\n",
      "Epoch 193: val_loss did not improve from 0.02691\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0068 - rmse_score: 0.2458 - mae_score: 0.0310 - val_loss: 0.0271 - val_rmse_score: 0.5112 - val_mae_score: 0.0778 - lr: 2.4363e-04\n",
      "Epoch 194/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0067 - rmse_score: 0.2448 - mae_score: 0.0308\n",
      "Epoch 194: val_loss did not improve from 0.02691\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0067 - rmse_score: 0.2448 - mae_score: 0.0308 - val_loss: 0.0271 - val_rmse_score: 0.5106 - val_mae_score: 0.0779 - lr: 2.4363e-04\n",
      "Epoch 195/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0067 - rmse_score: 0.2444 - mae_score: 0.0307\n",
      "Epoch 195: val_loss did not improve from 0.02691\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0067 - rmse_score: 0.2444 - mae_score: 0.0307 - val_loss: 0.0271 - val_rmse_score: 0.5099 - val_mae_score: 0.0774 - lr: 2.4363e-04\n",
      "Epoch 196/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0066 - rmse_score: 0.2431 - mae_score: 0.0304\n",
      "Epoch 196: val_loss did not improve from 0.02691\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0066 - rmse_score: 0.2431 - mae_score: 0.0304 - val_loss: 0.0269 - val_rmse_score: 0.5083 - val_mae_score: 0.0771 - lr: 2.4363e-04\n",
      "Epoch 197/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0065 - rmse_score: 0.2414 - mae_score: 0.0302\n",
      "Epoch 197: val_loss did not improve from 0.02691\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0065 - rmse_score: 0.2414 - mae_score: 0.0302 - val_loss: 0.0274 - val_rmse_score: 0.5127 - val_mae_score: 0.0781 - lr: 2.0708e-04\n",
      "Epoch 198/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0065 - rmse_score: 0.2402 - mae_score: 0.0300\n",
      "Epoch 198: val_loss improved from 0.02691 to 0.02682, saving model to ./models/trained_models/CASPIAN_beta_v4/initial\\\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0065 - rmse_score: 0.2402 - mae_score: 0.0300 - val_loss: 0.0268 - val_rmse_score: 0.5080 - val_mae_score: 0.0769 - lr: 2.0708e-04\n",
      "Epoch 199/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0065 - rmse_score: 0.2412 - mae_score: 0.0302\n",
      "Epoch 199: val_loss did not improve from 0.02682\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0065 - rmse_score: 0.2412 - mae_score: 0.0302 - val_loss: 0.0273 - val_rmse_score: 0.5127 - val_mae_score: 0.0778 - lr: 2.0708e-04\n",
      "Epoch 200/200\n",
      "1120/1120 [==============================] - ETA: 0s - loss: 0.0064 - rmse_score: 0.2398 - mae_score: 0.0299\n",
      "Epoch 200: val_loss did not improve from 0.02682\n",
      "1120/1120 [==============================] - 80s 72ms/step - loss: 0.0064 - rmse_score: 0.2398 - mae_score: 0.0299 - val_loss: 0.0276 - val_rmse_score: 0.5146 - val_mae_score: 0.0785 - lr: 2.0708e-04\n"
     ]
    }
   ],
   "source": [
    "name = MODEL_NAME+\"_split_{}_{}\".format(str(split), time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "#TF Callbacks\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='models/logs/{}'.format(name), histogram_freq=1)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"./models/trained_models/%s/initial/\" % MODEL_NAME, \n",
    "                    monitor=\"val_loss\", mode=\"min\", save_weights_only=True,\n",
    "                    save_best_only=True, verbose=1)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = EPOCHS//5)\n",
    "\n",
    "steps_per_epoch = int(ds[\"train\"].cardinality())#//batch_size\n",
    "lr_schedule = LinearDecayPerEpoch(LR, steps_per_epoch, EPOCHS, MIN_LR)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.85, patience=10, min_lr=MIN_LR)\n",
    "first_decay_steps = steps_per_epoch*(EPOCHS//3)\n",
    "cosine_lr_restarts = tf.keras.optimizers.schedules.CosineDecayRestarts(LR,first_decay_steps, t_mul=1.0, m_mul=0.8) \n",
    "\n",
    "# Compute the number of warmup batches\n",
    "warmup_batches = WARMUP_EPOCHS * steps_per_epoch\n",
    "# Create the Learning rate scheduler\n",
    "warm_up_lr = WarmUpLearningRateScheduler(warmup_batches, init_lr=LR)\n",
    "\n",
    "# Model Fitting\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "#opt = tfa.optimizers.AdamW(learning_rate=init_lr, weight_decay=0.0005)\n",
    "\n",
    "if \"CASP\" in MODEL_NAME:\n",
    "    if \"_\" not in MODEL_NAME:\n",
    "        model = new_models.CASPIAN(input_shape=(grid_size, grid_size, 1), filters=72, input_centering=True, \n",
    "                        depth=4, cardinality=34, activation='tanh', bottleneck_depth=8, \n",
    "                        init=\"glorot_normal\", sup_level=1, compression_factor= 0.85, gr_level= 4)\n",
    "    else:\n",
    "        model = new_models.CASPIAN_beta(input_shape=(grid_size, grid_size, 1), filters=72, input_centering=True, \n",
    "                depth=4, cardinality=34, activation='tanh', bottleneck_depth=8, \n",
    "                init=\"glorot_normal\", sup_level=1, compression_factor= 0.85, gr_level= 4, version=version)\n",
    "elif \"SWIN\" in MODEL_NAME:\n",
    "    set_seed()\n",
    "    model = new_models.Swin_unet(\n",
    "        filter_num_begin = 64, \n",
    "        depth = 4, \n",
    "        stack_num_down = 2, \n",
    "        stack_num_up = 2,\n",
    "        patch_size = 8,  \n",
    "        att_heads = 4,   \n",
    "        input_centering=True,\n",
    "        dropout = 0.0)\n",
    "else:\n",
    "    if \"pretrained\" in MODEL_NAME:\n",
    "        model = models.att_unet_2d((1024, 1024, 3), filter_num=[32, 64, 128, 256], n_labels=1, \n",
    "                               stack_num_down=2, stack_num_up=2, activation='ReLU', \n",
    "                               atten_activation='ReLU', attention='add', output_activation='ReLU', \n",
    "                               batch_norm=False, pool=True, unpool=False,\n",
    "                               backbone='VGG19', weights='imagenet', \n",
    "                               freeze_backbone=False, freeze_batch_norm=False, \n",
    "                               name='attunet')\n",
    "    else:\n",
    "        model = models.att_unet_2d((1024, 1024, 3), filter_num=[32, 64, 128, 256], n_labels=1, \n",
    "                               stack_num_down=2, stack_num_up=2, activation='ReLU', \n",
    "                               atten_activation='ReLU', attention='add', output_activation='ReLU', \n",
    "                               batch_norm=False, pool=True, unpool=False,\n",
    "                               backbone='VGG19', weights=None, \n",
    "                               freeze_backbone=False, freeze_batch_norm=False, \n",
    "                               name='attunet')\n",
    "\n",
    "\n",
    "if output_1d:\n",
    "    model.compile(loss=tf.keras.losses.Huber(delta=0.5), optimizer=opt, metrics=[tf.keras.metrics.RootMeanSquaredError(), \"mae\"])\n",
    "else:\n",
    "    model.compile(loss=custom_loss(mask=the_mask), optimizer=opt, metrics=[custom_rmse(mask=the_mask), custom_mae(mask=the_mask)])\n",
    "model.summary()\n",
    "\n",
    "history_warmup = model.fit(ds['train'],\n",
    "                epochs=WARMUP_EPOCHS,\n",
    "                validation_data=ds['val'],\n",
    "                callbacks=[checkpoint, tensorboard_callback, warm_up_lr]) #PrintLearningRate()#reduce_lr#early_stop\n",
    "\n",
    "model.load_weights(\"./models/trained_models/%s/initial/\" % MODEL_NAME)\n",
    "history = model.fit(ds['train'],\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=ds['val'],\n",
    "                callbacks=[checkpoint, tensorboard_callback, early_stop, reduce_lr]) #PrintLearningRate()#reduce_lr#early_stop\n",
    "\n",
    "model.load_weights(\"./models/trained_models/%s/initial/\" % MODEL_NAME)\n",
    "model.save(\"./models/trained_models/\"+MODEL_NAME+\"_split_{}\".format(str(split)), save_format='h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
