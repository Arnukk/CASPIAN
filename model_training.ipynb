{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89e9e3a-296a-4eb8-92d9-a0c3e16bd343",
      "metadata": {
        "id": "f89e9e3a-296a-4eb8-92d9-a0c3e16bd343"
      },
      "outputs": [],
      "source": [
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import time\n",
        "from models import models as new_models\n",
        "from utils import *\n",
        "import keras\n",
        "from keras_unet_collection import models, utils\n",
        "import matplotlib.pyplot as plt\n",
        "print('TensorFlow {}; Keras {}'.format(tf.__version__, keras.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f16a1eb-2ea8-470d-a1da-ec6e37692261",
      "metadata": {
        "id": "7f16a1eb-2ea8-470d-a1da-ec6e37692261"
      },
      "source": [
        "## Constants, Parameters and Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5846bdd3-227d-4aa7-9e15-8b4285ac3ba0",
      "metadata": {
        "id": "5846bdd3-227d-4aa7-9e15-8b4285ac3ba0"
      },
      "outputs": [],
      "source": [
        "grid_size = 1024\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 2\n",
        "split = 1\n",
        "output_1d = False\n",
        "EPOCHS = 200\n",
        "\n",
        "\n",
        "MODEL_NAME = \"Attn-Unet-pretrained\"\n",
        "LR = 0.00015\n",
        "MIN_LR = LR/5\n",
        "WARMUP_EPOCHS = 20\n",
        "\n",
        "MODEL_NAME = \"SWIN-Unet\"\n",
        "LR = 0.00018\n",
        "MIN_LR = LR/6\n",
        "WARMUP_EPOCHS = 20\n",
        "\n",
        "MODEL_NAME = \"CASPIAN\"\n",
        "LR = 0.0008\n",
        "MIN_LR = LR/10\n",
        "WARMUP_EPOCHS = 20\n",
        "\n",
        "\n",
        "###### For Ablation studies ###########################\n",
        "\n",
        "MODEL_NAME = \"CASPIAN_beta_v4\"\n",
        "LR = 0.0008\n",
        "MIN_LR = LR/10\n",
        "WARMUP_EPOCHS = 20\n",
        "version = 4\n",
        "###### For Ablation studies ###########################\n",
        "\n",
        "\n",
        "if not output_1d:\n",
        "    #Loading and processing the mask\n",
        "    the_mask = get_the_mask()\n",
        "\n",
        "\n",
        "#Loading the dataset\n",
        "ds = {\n",
        "    'train': tf.data.Dataset.load(\"./data/train_ds_aug_split_%d\" % split).map(lambda f,x,y,yf: tf.py_function(clear_ds,\n",
        "                                           inp=[f,x,y,yf, output_1d],\n",
        "                                           Tout=[tf.float32, tf.float32])),\n",
        "    'val': tf.data.Dataset.load(\"./data/val_ds_aug_split_%d\" % split).map(lambda f,x,y,yf: tf.py_function(clear_ds,\n",
        "                                           inp=[f,x,y,yf, output_1d],\n",
        "                                           Tout=[tf.float32, tf.float32]))\n",
        "}\n",
        "\n",
        "if \"Attn\" in MODEL_NAME:\n",
        "    ds['train'] = ds['train'].map(lambda x,y: expand_input_channels(x, y))\n",
        "    ds['val'] = ds['val'].map(lambda x,y: expand_input_channels(x, y))\n",
        "\n",
        "print(\"Size of the Training Dataset: %d  \" % tf.data.experimental.cardinality(ds[\"train\"]).numpy())\n",
        "print(\"Size of the Validation Dataset: %d  \" % tf.data.experimental.cardinality(ds[\"val\"]).numpy())\n",
        "input_sample, label_sample = next(iter(ds[\"train\"]))\n",
        "print(\"Training input shape: %s\" % str(input_sample.numpy().shape))\n",
        "print(\"Training input values: %s\" % str(np.unique(input_sample.numpy())))\n",
        "print(\"Training output shape: %s \" % str(label_sample.numpy().shape))\n",
        "input_sample, label_sample = next(iter(ds[\"val\"]))\n",
        "print(\"Validation input shape: %s\" % str(input_sample.numpy().shape))\n",
        "print(\"Validation output shape: %s \" % str(label_sample.numpy().shape))\n",
        "\n",
        "samples = 2\n",
        "fig, axs = plt.subplots(samples,2, facecolor='w', edgecolor='k')\n",
        "#fig.subplots_adjust(hspace = .2, wspace=.13)\n",
        "fig.tight_layout(pad=2, h_pad=2.5, w_pad=2.5)\n",
        "axs = axs.ravel()\n",
        "i = 0\n",
        "for element in list(ds['train'].as_numpy_iterator())[:samples]:\n",
        "    x,y = element\n",
        "    axs[i].imshow(x, origin=\"lower\", interpolation='none', aspect='auto')\n",
        "    axs[i].set_title(\"Sample input\")\n",
        "    i += 1\n",
        "    axs[i].imshow(y, origin=\"lower\", interpolation='none', aspect='auto')\n",
        "    axs[i].set_title(\"Sample output\")\n",
        "    i += 1\n",
        "\n",
        "ds['train'] = ds['train'].batch(batch_size)\n",
        "ds['val'] = ds['val'].batch(batch_size)\n",
        "\n",
        "the_mask = np.array([the_mask]*batch_size)\n",
        "print (\"Shape of the mask: %s\" % str(the_mask.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da54f2b-1030-496e-91c8-eefbb8155119",
      "metadata": {
        "id": "4da54f2b-1030-496e-91c8-eefbb8155119"
      },
      "outputs": [],
      "source": [
        "name = MODEL_NAME+\"_split_{}_{}\".format(str(split), time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "#TF Callbacks\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='models/logs/{}'.format(name), histogram_freq=1)\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"./models/trained_models/%s/initial/\" % MODEL_NAME,\n",
        "                    monitor=\"val_loss\", mode=\"min\", save_weights_only=True,\n",
        "                    save_best_only=True, verbose=1)\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = EPOCHS//5)\n",
        "\n",
        "steps_per_epoch = int(ds[\"train\"].cardinality())#//batch_size\n",
        "lr_schedule = LinearDecayPerEpoch(LR, steps_per_epoch, EPOCHS, MIN_LR)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.85, patience=10, min_lr=MIN_LR)\n",
        "first_decay_steps = steps_per_epoch*(EPOCHS//3)\n",
        "cosine_lr_restarts = tf.keras.optimizers.schedules.CosineDecayRestarts(LR,first_decay_steps, t_mul=1.0, m_mul=0.8)\n",
        "\n",
        "# Compute the number of warmup batches\n",
        "warmup_batches = WARMUP_EPOCHS * steps_per_epoch\n",
        "# Create the Learning rate scheduler\n",
        "warm_up_lr = WarmUpLearningRateScheduler(warmup_batches, init_lr=LR)\n",
        "\n",
        "# Model Fitting\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "#opt = tfa.optimizers.AdamW(learning_rate=init_lr, weight_decay=0.0005)\n",
        "\n",
        "if \"CASP\" in MODEL_NAME:\n",
        "    if \"_\" not in MODEL_NAME:\n",
        "        model = new_models.CASPIAN(input_shape=(grid_size, grid_size, 1), filters=72, input_centering=True,\n",
        "                        depth=4, cardinality=34, activation='tanh', bottleneck_depth=8,\n",
        "                        init=\"glorot_normal\", sup_level=1, compression_factor= 0.85, gr_level= 4)\n",
        "    else:\n",
        "        model = new_models.CASPIAN_beta(input_shape=(grid_size, grid_size, 1), filters=72, input_centering=True,\n",
        "                depth=4, cardinality=34, activation='tanh', bottleneck_depth=8,\n",
        "                init=\"glorot_normal\", sup_level=1, compression_factor= 0.85, gr_level= 4, version=version)\n",
        "elif \"SWIN\" in MODEL_NAME:\n",
        "    set_seed()\n",
        "    model = new_models.Swin_unet(\n",
        "        filter_num_begin = 64,\n",
        "        depth = 4,\n",
        "        stack_num_down = 2,\n",
        "        stack_num_up = 2,\n",
        "        patch_size = 8,\n",
        "        att_heads = 4,\n",
        "        input_centering=True,\n",
        "        dropout = 0.0)\n",
        "else:\n",
        "    if \"pretrained\" in MODEL_NAME:\n",
        "        model = models.att_unet_2d((1024, 1024, 3), filter_num=[32, 64, 128, 256], n_labels=1,\n",
        "                               stack_num_down=2, stack_num_up=2, activation='ReLU',\n",
        "                               atten_activation='ReLU', attention='add', output_activation='ReLU',\n",
        "                               batch_norm=False, pool=True, unpool=False,\n",
        "                               backbone='VGG19', weights='imagenet',\n",
        "                               freeze_backbone=False, freeze_batch_norm=False,\n",
        "                               name='attunet')\n",
        "    else:\n",
        "        model = models.att_unet_2d((1024, 1024, 3), filter_num=[32, 64, 128, 256], n_labels=1,\n",
        "                               stack_num_down=2, stack_num_up=2, activation='ReLU',\n",
        "                               atten_activation='ReLU', attention='add', output_activation='ReLU',\n",
        "                               batch_norm=False, pool=True, unpool=False,\n",
        "                               backbone='VGG19', weights=None,\n",
        "                               freeze_backbone=False, freeze_batch_norm=False,\n",
        "                               name='attunet')\n",
        "\n",
        "\n",
        "if output_1d:\n",
        "    model.compile(loss=tf.keras.losses.Huber(delta=0.5), optimizer=opt, metrics=[tf.keras.metrics.RootMeanSquaredError(), \"mae\"])\n",
        "else:\n",
        "    model.compile(loss=custom_loss(mask=the_mask), optimizer=opt, metrics=[custom_rmse(mask=the_mask), custom_mae(mask=the_mask)])\n",
        "model.summary()\n",
        "\n",
        "history_warmup = model.fit(ds['train'],\n",
        "                epochs=WARMUP_EPOCHS,\n",
        "                validation_data=ds['val'],\n",
        "                callbacks=[checkpoint, tensorboard_callback, warm_up_lr]) #PrintLearningRate()#reduce_lr#early_stop\n",
        "\n",
        "model.load_weights(\"./models/trained_models/%s/initial/\" % MODEL_NAME)\n",
        "history = model.fit(ds['train'],\n",
        "                epochs=EPOCHS,\n",
        "                validation_data=ds['val'],\n",
        "                callbacks=[checkpoint, tensorboard_callback, early_stop, reduce_lr]) #PrintLearningRate()#reduce_lr#early_stop\n",
        "\n",
        "model.load_weights(\"./models/trained_models/%s/initial/\" % MODEL_NAME)\n",
        "model.save(\"./models/trained_models/\"+MODEL_NAME+\"_split_{}\".format(str(split)), save_format='h5')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}